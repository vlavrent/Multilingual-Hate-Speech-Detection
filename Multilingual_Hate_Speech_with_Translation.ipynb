{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multilingual Hate Speech with Translation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNjulO5n/Plr4UEDPOP9d+2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlavrent/Multilingual-Hate-Speech-Detection/blob/main/Multilingual_Hate_Speech_with_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1sHqRCn_csZ"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "id": "GOwBGl3L_ipB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece"
      ],
      "metadata": {
        "id": "tiWaOx-J_kDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rte0a5w0_mgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "  print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "IA_mgcjY_nIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#from torchsampler import ImbalancedDatasetSampler\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "from sklearn.model_selection import KFold\n",
        "import torch, gc\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from sklearn.metrics import f1_score,classification_report\n",
        "from transformers import RobertaTokenizer, RobertaModel, CamembertTokenizer, CamembertModel\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "GhOv9uxv_ofo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Polish():\n",
        "\n",
        "    def __init__(self,data_path,tag_path):\n",
        "        self.path = data_path\n",
        "        self.tag_path = tag_path\n",
        "        self.data = self.read_data()\n",
        "        self.tag = self.read_tag()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "\n",
        "        open_data = open(self.path, \"r\", encoding=\"utf8\")\n",
        "\n",
        "        return pd.DataFrame(open_data)\n",
        "\n",
        "    def read_tag(self):\n",
        "\n",
        "        open_tag = open(self.tag_path,'r')\n",
        "\n",
        "        return pd.DataFrame(open_tag)\n",
        "\n",
        "    def remove_punctuation(self,data,column):\n",
        "      \n",
        "      return data[column].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\n",
        "  \n",
        "    def lower(self):\n",
        "\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def rename_columns(self,data,column):\n",
        "\n",
        "        return data.rename(columns={0:column})\n",
        "\n",
        "    def remove_url(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'http\\S+', '',x))\n",
        "\n",
        "    def remove_end_line(self,data,column):\n",
        "\n",
        "        return data[column].str.replace('\\n','')\n",
        "\n",
        "    def concat(self):\n",
        "\n",
        "        self.data[self.label] = self.tag[self.label]\n",
        "\n",
        "        return self.data\n",
        "    \n",
        "    def convert_int(self):\n",
        "\n",
        "      return self.tag[self.label].apply(lambda x: int(x))\n",
        "\n",
        "    def double_space(self):\n",
        "      self.data[self.column].apply(lambda x: x.replace('  ',' '))\n",
        "\n",
        "\n",
        "    def clean_data(self):\n",
        "\n",
        "        text_column = self.column\n",
        "        label_column = self.label\n",
        "\n",
        "        # Rename columns in both label and text data\n",
        "        self.data = self.rename_columns(self.data,text_column)\n",
        "\n",
        "        self.tag = self.rename_columns(self.tag, label_column)\n",
        "\n",
        "        # Remove url\n",
        "        self.remove_url()\n",
        "\n",
        "        # Remove end line character from label and text data\n",
        "        self.data[text_column] = self.remove_end_line(self.data,text_column)\n",
        "\n",
        "        self.tag[label_column] = self.remove_end_line(self.tag,label_column)\n",
        "\n",
        "\n",
        "        # Remove Punctuation\n",
        "        self.data[text_column] = self.remove_punctuation(self.data,text_column) \n",
        "\n",
        "        # Lower words in text data\n",
        "        self.data[text_column] = self.lower()\n",
        "\n",
        "        # Remove double space\n",
        "        self.double_space()\n",
        "\n",
        "        # Convert label to int\n",
        "        self.tag[label_column] = self.convert_int()\n",
        "        \n",
        "        # Concat text and labels\n",
        "\n",
        "        return self.concat()"
      ],
      "metadata": {
        "id": "d0SHcOUy_qCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Slovenian():\n",
        "    def __init__(self,path):\n",
        "        self.path = path\n",
        "        self.data = self.read_data()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "        return pd.read_csv(self.path)\n",
        "\n",
        "    def rename_columns(self):\n",
        "\n",
        "        self.data = self.data[['text','type']]\n",
        "        return self.data.rename(columns={'type':self.label})\n",
        "    \n",
        "    def remove_url(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'http\\S+', '',x))\n",
        "\n",
        "    def strip_punctuation(self):\n",
        "        return self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
        "\n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def remove_stopwords(self, data, column):\n",
        "        data[column] = data[column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"slovene\")])\n",
        "        return data[column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def change_labels(self,x):\n",
        "\n",
        "        change = {'Background offensive':'offensive', 'Acceptable speech':'acceptable', 'Background violence':'offensive',\n",
        "                  'Other offensive':'offensive', 'Inappropriate':'offensive', 'Other violence':'offensive'}\n",
        "\n",
        "        for k, v in change.items():\n",
        "\n",
        "             x = x.replace(k, v)\n",
        "        return x\n",
        "\n",
        "    def convert_labels(self):\n",
        "\n",
        "        self.data[self.label] = self.data[self.label].apply(lambda x: self.change_labels(x))\n",
        "\n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 1 if x=='offensive' else 0)\n",
        "    \n",
        "    def double_space(self):\n",
        "      self.data[self.column].apply(lambda x: x.replace('  ',' '))\n",
        "\n",
        "\n",
        "    def clean_data(self):\n",
        "\n",
        "        # Rename Columns\n",
        "        self.data = self.rename_columns()\n",
        "\n",
        "        # Remove url\n",
        "        self.remove_url()\n",
        "\n",
        "        # Strip Punctuation\n",
        "        self.data[self.column] = self.strip_punctuation()\n",
        "\n",
        "        # Lowercase\n",
        "        self.data[self.column] = self.lower()\n",
        "\n",
        "        # Remove stopwords\n",
        "        self.data[self.column] = self.remove_stopwords(self.data,self.column)\n",
        "        \n",
        "        # Convert labels\n",
        "        self.convert_labels()\n",
        "\n",
        "        # Remove double space\n",
        "        self.double_space()\n",
        "        \n",
        "        # Binarize labels\n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "metadata": {
        "id": "KcQ4TjRr_7CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Croatian():\n",
        "    def __init__(self,path):\n",
        "        self.path = path\n",
        "        self.data = self.read_data()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "        return pd.read_csv(self.path)\n",
        "\n",
        "    def rename_columns(self):\n",
        "\n",
        "        self.data = self.data[['text','type']]\n",
        "        return self.data.rename(columns={'type':self.label})\n",
        "    \n",
        "    def remove_url(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'http\\S+', '',x))\n",
        "\n",
        "    def strip_punctuation(self):\n",
        "        return self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
        "\n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def remove_stopwords(self, data, column):\n",
        "        data[column] = data[column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"slovene\")])\n",
        "        return data[column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def change_labels(self,x):\n",
        "\n",
        "        change = {'Background offensive':'offensive', 'Acceptable speech':'not offensive', 'Background violence':'offensive',\n",
        "                  'Other offensive':'offensive', 'Inappropriate':'offensive', 'Other violence':'offensive'}\n",
        "\n",
        "        for k, v in change.items():\n",
        "\n",
        "             x = x.replace(k, v)\n",
        "        return x\n",
        "\n",
        "    def convert_labels(self):\n",
        "\n",
        "        self.data[self.label] = self.data[self.label].apply(lambda x: self.change_labels(x))\n",
        "\n",
        "        hate = ['offensive','not offensive']\n",
        "\n",
        "        self.data = self.data[self.data[self.label].isin(hate)]\n",
        "    \n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 1 if x=='offensive' else 0)\n",
        "\n",
        "    def double_space(self):\n",
        "      self.data[self.column].apply(lambda x: x.replace('  ',' '))\n",
        "\n",
        "    def clean_data(self):\n",
        "\n",
        "        # Rename Columns\n",
        "        self.data = self.rename_columns()\n",
        "\n",
        "        # Remove url\n",
        "        self.remove_url()\n",
        "\n",
        "        # Strip Punctuation\n",
        "        self.data[self.column] = self.strip_punctuation()\n",
        "\n",
        "        # Lowercase\n",
        "        self.data[self.column] = self.lower()\n",
        "\n",
        "        # Remove stopwords\n",
        "        self.data[self.column] = self.remove_stopwords(self.data,self.column)\n",
        "\n",
        "        # Remove double space\n",
        "        self.double_space()\n",
        "        \n",
        "        # Binarize labels\n",
        "        self.convert_labels()\n",
        "       \n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "metadata": {
        "id": "Ofi1bpjbAFJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "class Greek():\n",
        "    def __init__(self,path):\n",
        "        self.path  = path\n",
        "        self.data = self.read_data()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "        self.data = pd.read_csv(self.path)\n",
        "        self.data.tweet = self.data.tweet.apply(lambda x: str(x))\n",
        "        self.data.subtask_a = self.data.subtask_a.apply(lambda x: str(x))\n",
        "        return self.data\n",
        "\n",
        "    def rename_columns(self):\n",
        "\n",
        "        return self.data.rename(columns={'tweet':self.column,'subtask_a':self.label})\n",
        "\n",
        "    def replace_hashtag(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(\"#[\\w]+\", \"hashtag \", x))\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: x.replace('hashtag',''))\n",
        "        return self.data[self.column]\n",
        "    \n",
        "    def remove_url(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'http\\S+', '',x))\n",
        "\n",
        "    def strip_punctuation(self):\n",
        "\n",
        "        return self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\n",
        "\n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def remove_stopwords(self, data, column):\n",
        "        data[column] = data[column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"greek\")])\n",
        "        return data[column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 0 if x=='NOT' else 1)\n",
        "    \n",
        "    def double_space(self):\n",
        "      self.data[self.column].apply(lambda x: x.replace('  ',' '))\n",
        "\n",
        "    def clean_data(self):\n",
        "        text_column = self.column\n",
        "        label_column = self.label\n",
        "\n",
        "        # Rename Columns\n",
        "        self.data = self.rename_columns()\n",
        "\n",
        "        # Replace hashtag\n",
        "        self.data[text_column] = self.replace_hashtag()\n",
        "\n",
        "        # Remove urls\n",
        "        self.remove_url()\n",
        "\n",
        "        # Strip Punctuation\n",
        "        self.data[text_column] = self.strip_punctuation()\n",
        "\n",
        "        # Lower text\n",
        "        self.data[text_column] = self.lower()\n",
        "\n",
        "        # Remove Stopwords\n",
        "        self.data[text_column] = self.remove_stopwords(self.data,text_column)\n",
        "\n",
        "        # Remove double space\n",
        "        self.double_space()\n",
        "\n",
        "        # Binarize labels\n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "metadata": {
        "id": "RfFIjzxpAGtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class English():\n",
        "\n",
        "    def __init__(self,path):\n",
        "        self.data = self.read(path)\n",
        "        self.label = 'label'\n",
        "        self.column = 'text'\n",
        "\n",
        "    def read(self,path):\n",
        "        return pd.read_csv(path)\n",
        "\n",
        "    def replace_mentions(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda row: re.sub(\"@[A-Za-z0-9]+_*[A-Za-z0-9]+|@[_]+[A-Za-z0-9]+_*[A-Za-z0-9]+|@[_]+[A-Za-z0-9]+[_]+_*[A-Za-z0-9]+[_]+\", \"mention\", row))\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda row: re.sub(\"mention_\", \"mention\", row))\n",
        "\n",
        "    def remove_end_line(self):\n",
        "\n",
        "        return self.data[self.column].str.replace('\\n','')\n",
        "\n",
        "    def remove_punctuation(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "    def replace_hashtag(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(\"#[\\w]+\", \"hashtag \", x))\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: x.replace('hashtag',''))\n",
        "        \n",
        "    def remove_stopwords(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"english\")])\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def remove_url(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%|\\#)*\\b', '',x))\n",
        "    \n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "    \n",
        "    def double_space(self):\n",
        "      return self.data[self.column].apply(lambda x: x.replace('  ',' '))\n",
        "    \n",
        "    def remove_noise(self):\n",
        "      return self.data[self.column].apply(lambda x: x.replace('rt',''))\n",
        "\n",
        "    \n",
        "    def clean_data(self):\n",
        "      \n",
        "        # Remove urls\n",
        "        self.remove_url()\n",
        "\n",
        "        # Remove end-line\n",
        "        self.data[self.column] = self.remove_end_line()\n",
        "\n",
        "        # Replace mentions\n",
        "        self.replace_mentions()\n",
        "\n",
        "        # Replace hashtags\n",
        "        self.replace_hashtag()\n",
        "\n",
        "        # Remove punctuation\n",
        "        self.remove_punctuation()\n",
        "\n",
        "        # Remove stopwords\n",
        "        self.remove_stopwords()\n",
        "\n",
        "        # Lower text\n",
        "        self.data[self.column] = self.lower()\n",
        "\n",
        "        # Remove double space\n",
        "        self.data[self.column] = self.double_space()\n",
        "\n",
        "        # Remove noise\n",
        "        self.data[self.column] = self.remove_noise()\n",
        "\n",
        "        return self.data"
      ],
      "metadata": {
        "id": "cg6RUeUBAauB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transform_Data():\n",
        "  def __init__(self,train,test,language_model,column,label,cmodel):\n",
        "    self.max_length = 60\n",
        "    self.tokenizer = cmodel.from_pretrained(language_model)\n",
        "    self.train = train\n",
        "    self.test = test\n",
        "    self.column = column\n",
        "    self.label = label\n",
        "\n",
        "  def Tokenizer(self):\n",
        "\n",
        "    train_encodings = self.tokenizer.batch_encode_plus(self.train[self.column].tolist(),add_special_tokens = True, truncation=True, padding=True, max_length=self.max_length,return_tensors='pt')\n",
        "    train_y = torch.tensor(self.train[self.label].tolist())\n",
        "  \n",
        "    val_encodings = self.tokenizer.batch_encode_plus(self.test[self.column].tolist(),add_special_tokens = True, truncation=True, padding=True, max_length=self.max_length,return_tensors='pt')\n",
        "    val_y = torch.tensor(self.test[self.label].tolist())\n",
        "\n",
        "    return train_encodings, train_y, val_encodings,val_y \n",
        "\n",
        "  def Tensors_and_DataLoaders(self,train_encodings,train_y,val_encodings, val_y):\n",
        "\n",
        "    #====================\n",
        "    #Train data\n",
        "    #====================\n",
        "    train_data = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_y)\n",
        "    class_samples = [(train_y == 0.).sum(dim=0),(train_y == 1.).sum(dim=0)]\n",
        "    total_samples = sum(class_samples)\n",
        "    \n",
        "\n",
        "    class_weights = [total_samples/class_samples[i] for i in range(len(class_samples))]\n",
        "    weights = [class_weights[train_y[i]] for i in range(int(total_samples))]\n",
        "\n",
        "    train_sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(total_samples))\n",
        "    \n",
        "\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "\n",
        "    #====================\n",
        "    #Test/Validation data\n",
        "    #====================\n",
        "\n",
        "    val_data = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_y)\n",
        "\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=32)\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "  def Execute(self):\n",
        "\n",
        "    train_encodings, train_y, val_encodings,val_y = self.Tokenizer()\n",
        "    train_dataloader, val_dataloader = self.Tensors_and_DataLoaders(train_encodings,train_y,val_encodings, val_y)\n",
        "    return train_encodings, train_y, val_encodings,val_y, train_dataloader, val_dataloader\n"
      ],
      "metadata": {
        "id": "CKb186-GAci2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Model_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, ROBERTA,freeze_bert):\n",
        "      \n",
        "      super(Model_Arch, self).__init__()\n",
        "\n",
        "      self.bert = ROBERTA\n",
        "      self.freeze_bert = freeze_bert\n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.3)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(self.bert.config.hidden_size,1)\n",
        "      \n",
        "\n",
        "      # sigmoid activation function\n",
        "      self.sigmoid =  nn.Sigmoid()\n",
        "      #self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "      if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _,cls_hs = self.bert(sent_id, attention_mask=mask,return_dict=False) #,return_dict=False\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "      \n",
        "      # activation function\n",
        "      #x = self.relu(x)\n",
        "      \n",
        "      # dropout\n",
        "      #x = self.dropout(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.sigmoid(x)\n",
        "      \n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "XoPqzKyQAeUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import get_scheduler\n",
        "\n",
        "\n",
        "def Optimizer_Scheduler(model,train_dataloader,num_epochs):\n",
        "  \n",
        "  optimizer = AdamW(model.parameters(), lr=5e-5) #weight_decay=0.02\n",
        " \n",
        "  num_training_steps = num_epochs * len(train_dataloader)\n",
        "  lr_scheduler = get_linear_schedule_with_warmup(\n",
        "                       optimizer=optimizer,\n",
        "                       num_warmup_steps=0,\n",
        "                       num_training_steps=num_training_steps)\n",
        "\n",
        "  return optimizer, lr_scheduler, num_training_steps"
      ],
      "metadata": {
        "id": "JhXZuo7aAgOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def Mean(data):\n",
        "    return sum(data) / len(data)\n",
        "\n",
        "def train(model, num_epochs, train_dataloader,val_dataloader,best_valid_loss,language,lanmodel):\n",
        "\n",
        "  # Optimizer and Scheduler\n",
        "  optimizer, lr_scheduler, num_training_steps = Optimizer_Scheduler(model,train_dataloader,num_epochs)\n",
        "\n",
        "  progress_bar = tqdm(range(num_training_steps))\n",
        "  loss_fn = nn.BCELoss()\n",
        "  #loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "  # Initialize arrays\n",
        "  train_acc = []\n",
        "  val_acc = [] \n",
        "  train_loss = []  \n",
        "  val_loss = []\n",
        "  avg_acc_0 = []\n",
        "  avg_acc_1 = []\n",
        "  total_avg_loss_train = []\n",
        "  total_avg_loss_val = []\n",
        "\n",
        "  # Set a flag for results\n",
        "  flag = True\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "    \n",
        "    total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "    predictions = []\n",
        "    real_label = []\n",
        "\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      batch_counts += 1\n",
        "      \n",
        "      # batch to GPU\n",
        "      batch = [r.to(device) for r in batch]\n",
        "\n",
        "      sent_id, mask, labels = batch\n",
        "      real_label.append(labels.detach().cpu().numpy())\n",
        "      \n",
        "      \n",
        "      # clear previous gradients\n",
        "      model.zero_grad()\n",
        "       \n",
        "      # predictions for current batch\n",
        "      output = model(sent_id, mask)\n",
        "\n",
        "      pred = torch.round(output)\n",
        "      #pred = torch.argmax(output,1)\n",
        "\n",
        "      pred = pred.detach().cpu().numpy()\n",
        "      pred = pred.flatten()\n",
        "      predictions.append(pred)\n",
        "\n",
        "      \n",
        "      \n",
        "      # compute loss for current batch\n",
        "      loss = loss_fn(output, labels.unsqueeze(1).float())\n",
        "      #loss = loss_fn(output, labels)\n",
        "\n",
        "      # add total loss\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "\n",
        "      # backpropagation to calculate gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # prevent the exploding gradient problem\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "\n",
        "      # update parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      # update scheduler \n",
        "      lr_scheduler.step()\n",
        "\n",
        "      # clear optimizer gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      progress_bar.update(1)\n",
        "\n",
        "    # compute training loss of each batch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # flatten labels and predictions\n",
        "    flat_label = np.concatenate(real_label).astype(int).ravel().tolist()\n",
        "    \n",
        "    flat_predictions = np.concatenate(predictions).astype(int).ravel().tolist()\n",
        "\n",
        "    # Append accuracy and validation loss for training data\n",
        "    train_acc.append(f1_score(flat_label,flat_predictions))\n",
        "    train_loss.append(avg_loss)\n",
        "\n",
        "    # Validation\n",
        "    val_flat_label,val_flat_predictions, val_avg_loss = validate(model,val_dataloader,loss_fn)\n",
        "\n",
        "    if val_avg_loss < best_valid_loss:\n",
        "        best_valid_loss = val_avg_loss\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/Datasets/All/'+lanmodel+'_'+language+'_weights.pt')     \n",
        "    \n",
        "    \n",
        "    # Append accuracy and validation loss for validation data\n",
        "    val_acc.append(f1_score(val_flat_label,val_flat_predictions))\n",
        "    val_loss.append(val_avg_loss)\n",
        "\n",
        "    # Compute each class Accuracy (validation set)\n",
        "    acc_0,acc_1 = Class_F1_score(val_flat_label,val_flat_predictions)\n",
        "    avg_acc_0.append(acc_0)\n",
        "    avg_acc_1.append(acc_1)\n",
        "\n",
        "    # Print table with insights\n",
        "    if flag:\n",
        "      print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Train F1-score':^9} | {'Val F1-score':^9} | {'Class_0 F1-score':^10} | {'Class_1 F1-score':^10} \")\n",
        "      flag = False\n",
        "\n",
        "    print(\"-\"*85)\n",
        "    print(f\"{str(epoch + 1) +'/'+ str(num_epochs):^7} | {avg_loss:^12.6f} | {val_avg_loss:^10.6f} | {f1_score(flat_label,flat_predictions):^9.2f} | {f1_score(val_flat_label,val_flat_predictions):^9.2f} | {acc_0:^10.2f} | {acc_1:^10.2f} \")\n",
        "  \n",
        "  \n",
        "\n",
        "  # Print mean of Accuracy and Loss\n",
        "  print(\"-\"*85)\n",
        "  print(f\"{'Average':^7} | {Mean(train_loss):^12.6f} | {Mean(val_loss):^10.6f} | {Mean(train_acc):^9.2f} | {Mean(val_acc):^9.2f} | {Mean(avg_acc_0):^10.2f} | {Mean(avg_acc_1):^10.2f} \")\n",
        "\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pt99HdT4Aiqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model,val_dataloader,loss_fn):\n",
        "  \n",
        "    \n",
        "\n",
        "    model.eval() \n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    \n",
        "    val_preds = []\n",
        "    val_label = []\n",
        "    \n",
        "    for step,batch in enumerate(val_dataloader):\n",
        "      \n",
        "      batch = [t.to(device) for t in batch]\n",
        "      \n",
        "      sent_id, mask, labels = batch\n",
        "      val_label.append(labels.detach().cpu().numpy())\n",
        "      \n",
        "      with torch.no_grad():\n",
        "        output = model(sent_id,mask)\n",
        "        \n",
        "      loss = loss_fn(output,labels.unsqueeze(1).float())  \n",
        "      #loss = loss_fn(output,labels) \n",
        "      total_loss = total_loss + loss.item()\n",
        "        \n",
        "        \n",
        "      pred = torch.round(output)\n",
        "      #pred = torch.argmax(output,1)\n",
        "      pred = pred.detach().cpu().numpy()\n",
        "      pred = pred.flatten()\n",
        "      val_preds.append(pred)\n",
        "\n",
        "    # compute training loss of each epoch\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "    print('Total Loss: '+str(total_loss))\n",
        "    print('Avg Loss: '+str(avg_loss))\n",
        "\n",
        "    # flatten labels and predictions\n",
        "    flat_label = np.concatenate(val_label).astype(int).ravel().tolist()\n",
        "    \n",
        "    \n",
        "    flat_predictions = np.concatenate(val_preds).astype(int).ravel().tolist()\n",
        "\n",
        "    \n",
        "\n",
        "    return flat_label,flat_predictions, avg_loss"
      ],
      "metadata": {
        "id": "wCA4pF3AAkWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Class_F1_score(val_y,new_preds):\n",
        "\n",
        "  report = classification_report(val_y,new_preds, output_dict=True )\n",
        "\n",
        "  return report['0']['f1-score'], report['1']['f1-score']"
      ],
      "metadata": {
        "id": "IpkWiKebAl-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "import numpy as np\n",
        "import random\n",
        "from past.builtins import xrange\n",
        "\n",
        "def kMedoids(D, k, tmax=100):\n",
        "    # determine dimensions of distance matrix D\n",
        "    m, n = D.shape\n",
        "\n",
        "    if k > n:\n",
        "        raise Exception('too many medoids')\n",
        "\n",
        "    # find a set of valid initial cluster medoid indices since we\n",
        "    # can't seed different clusters with two points at the same location\n",
        "    valid_medoid_inds = set(range(n))\n",
        "    invalid_medoid_inds = set([])\n",
        "    rs,cs = np.where(D==0)\n",
        "    # the rows, cols must be shuffled because we will keep the first duplicate below\n",
        "    index_shuf = list(range(len(rs)))\n",
        "    np.random.shuffle(index_shuf)\n",
        "    rs = rs[index_shuf]\n",
        "    cs = cs[index_shuf]\n",
        "    for r,c in zip(rs,cs):\n",
        "        # if there are two points with a distance of 0...\n",
        "        # keep the first one for cluster init\n",
        "        if r < c and r not in invalid_medoid_inds:\n",
        "            invalid_medoid_inds.add(c)\n",
        "    valid_medoid_inds = list(valid_medoid_inds - invalid_medoid_inds)\n",
        "\n",
        "    if k > len(valid_medoid_inds):\n",
        "        raise Exception('too many medoids (after removing {} duplicate points)'.format(\n",
        "            len(invalid_medoid_inds)))\n",
        "\n",
        "    # randomly initialize an array of k medoid indices\n",
        "    M = np.array(valid_medoid_inds)\n",
        "    np.random.shuffle(M)\n",
        "    M = np.sort(M[:k])\n",
        "\n",
        "    # create a copy of the array of medoid indices\n",
        "    Mnew = np.copy(M)\n",
        "\n",
        "    # initialize a dictionary to represent clusters\n",
        "    C = {}\n",
        "    for t in xrange(tmax):\n",
        "        # determine clusters, i. e. arrays of data indices\n",
        "        J = np.argmin(D[:,M], axis=1)\n",
        "        for kappa in range(k):\n",
        "            C[kappa] = np.where(J==kappa)[0]\n",
        "        # update cluster medoids\n",
        "        for kappa in range(k):\n",
        "            J = np.mean(D[np.ix_(C[kappa],C[kappa])],axis=1)\n",
        "            j = np.argmin(J)\n",
        "            Mnew[kappa] = C[kappa][j]\n",
        "        np.sort(Mnew)\n",
        "        # check for convergence\n",
        "        if np.array_equal(M, Mnew):\n",
        "            break\n",
        "        M = np.copy(Mnew)\n",
        "    else:\n",
        "        # final update of cluster memberships\n",
        "        J = np.argmin(D[:,M], axis=1)\n",
        "        for kappa in range(k):\n",
        "            C[kappa] = np.where(J==kappa)[0]\n",
        "\n",
        "    # return results\n",
        "    return M, C\n",
        "\n",
        "def balance(data,sample):\n",
        "\n",
        "      model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "      embeddings = model.encode(data['text'].to_numpy())\n",
        "\n",
        "      D = pairwise_distances(embeddings, metric='cosine')\n",
        "\n",
        "      M, C = kMedoids(D, sample)\n",
        "      return data.iloc[M]\n",
        "\n",
        "\n",
        "\n",
        "def execute(data,sample):\n",
        "   \n",
        "    return balance(data,sample)"
      ],
      "metadata": {
        "id": "NjwWR_PhAnfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def find_length(language):\n",
        "  length = []\n",
        "  \n",
        "  for count, value in enumerate(language):\n",
        "    \n",
        "    if value=='slovenian' or value=='Slovenian':\n",
        "      # Slovenian\n",
        "      text_path = \"/content/drive/My Drive/Datasets/Slovenian/Slovene_train_set.csv\"\n",
        "      length.append(len(pd.read_csv(text_path).index))\n",
        "\n",
        "    elif value=='polish' or value=='Polish':\n",
        "      # Polish\n",
        "      text_path = \"/content/drive/My Drive/Datasets/Polish/training_set_clean_only_text.txt\"\n",
        "      tag_path = \"/content/drive/My Drive/Datasets/Polish/training_set_clean_only_tags.txt\"\n",
        "      length.append(len(pd.read_csv(open(tag_path, \"r\", encoding=\"utf8\")).index))\n",
        "      \n",
        "    elif value=='croatian' or value=='Croatian':\n",
        "      # Croatian\n",
        "      text_path = \"/content/drive/My Drive/Datasets/Croatian/Croatian_train_set.csv\"\n",
        "      length.append(len(pd.read_csv(text_path).index))\n",
        "\n",
        "    elif value=='greek' or value=='Greek':\n",
        "      # Greek\n",
        "      text_path = \"/content/drive/My Drive/Datasets/Greek/offenseval-gr_train.csv\"\n",
        "      \n",
        "      length.append(len(pd.read_csv(text_path).index))\n",
        "  return min(length)\n",
        " \n",
        "\n",
        "\n",
        "def preprocess_language(language,length,number_language):\n",
        "\n",
        "  if language=='slovenian' or language=='Slovenian':\n",
        "    # Slovenian\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Slovenian/Slovene_train_set.csv\"\n",
        "\n",
        "    slovenian = Slovenian(text_path)\n",
        "    data = slovenian.clean_data()\n",
        "\n",
        "  elif language=='polish' or language=='Polish':\n",
        "    # Polish\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Polish/training_set_clean_only_text.txt\"\n",
        "    tag_path = \"/content/drive/My Drive/Datasets/Polish/training_set_clean_only_tags.txt\"\n",
        "    translate = \"/content/drive/My Drive/Datasets/Polish/Polish_translated.csv\"\n",
        "\n",
        "    polish = Polish(text_path,tag_path,translate)\n",
        "    data = polish.clean_data()\n",
        "    print(data)\n",
        "    print(data.label.value_counts())\n",
        "    if number_language>1 and len(data)>length:\n",
        "          data  = execute(data,length)\n",
        "  \n",
        "  elif language=='croatian' or language=='Croatian':\n",
        "    # Croatian\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Croatian/Croatian_train_set.csv\"\n",
        "\n",
        "    croatian = Croatian(text_path)\n",
        "    data = croatian.clean_data()\n",
        "    if number_language>1 and len(data)>length:\n",
        "          data  = execute(data,length)\n",
        "\n",
        "  elif language=='greek' or language=='Greek':\n",
        "    # Greek\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Greek/offenseval-gr_train-translated.csv\"\n",
        "\n",
        "    greek = Greek(text_path)\n",
        "    data = greek.clean_data()\n",
        "    print(data)\n",
        "    if number_language>1 and len(data)>length:\n",
        "      data  = execute(data,length)\n",
        "  elif language=='english' or language=='English':\n",
        "    # English\n",
        "    text_path = \"/content/drive/My Drive/Datasets/English/English_12514.csv\"\n",
        "\n",
        "    english = English(text_path)\n",
        "    data = english.clean_data()\n",
        "    print(data)\n",
        "\n",
        "    #if number_language>1 and len(data)>length:\n",
        "     # data  = execute(data,12514)\n",
        "  return data\n",
        "\n",
        "def languages(lang):\n",
        "  \n",
        "  total_languages = []\n",
        "  length = find_length(lang)\n",
        "  print(len(lang))\n",
        "  \n",
        "\n",
        "  for count, value in enumerate(lang):\n",
        "    \n",
        "    total_languages.append(preprocess_language(value,length,len(lang)))\n",
        "  \n",
        "  return pd.concat(total_languages)\n"
      ],
      "metadata": {
        "id": "EM7TvW8UBGwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_model(cmodel):\n",
        "  if cmodel=='Bert' or cmodel=='BERT' or cmodel=='BERT' or cmodel=='bert':\n",
        "    return BertModel, BertTokenizer\n",
        "  elif cmodel=='ROBERTA' or cmodel=='XLMRoberta' or cmodel=='Roberta' or cmodel=='roberta':\n",
        "    return XLMRobertaModel, XLMRobertaTokenizer\n",
        "  elif cmodel=='camembert' or cmodel=='Camembert':\n",
        "    return CamembertModel, CamembertTokenizer"
      ],
      "metadata": {
        "id": "XUPs5-B7BMnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def language_model(mlang):\n",
        "\n",
        "  if mlang=='multilingual_roberta':\n",
        "    return \"xlm-roberta-base\"\n",
        "\n",
        "  elif mlang=='bert_greek':\n",
        "    return \"nlpaueb/bert-base-greek-uncased-v1\"\n",
        "\n",
        "  elif mlang=='bert_polish':\n",
        "    return \"dkleczek/bert-base-polish-uncased-v1\"\n",
        "  \n",
        "  elif mlang=='bert_croatian':\n",
        "    return \"EMBEDDIA/crosloengual-bert\"\n",
        "  \n",
        "  elif mlang=='bert_slovenian':\n",
        "    return \"EMBEDDIA/sloberta\"\n",
        "  \n",
        "  elif mlang=='multilingual_bert':\n",
        "    return \"bert-base-multilingual-cased\""
      ],
      "metadata": {
        "id": "fQddBQY6BOTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "settings = {\n",
        "    'language_model':'multilingual_bert', #multilingual_roberta, multilingual_bert, bert_greek, bert_polish, bert_croatian, bert_slovenian\n",
        "    'model': 'bert',                #roberta, bert, camembert\n",
        "    'num_epochs': 5,\n",
        "    'training_language': ['polish','croatian'] #greek, english, slovenian, polish, croatian\n",
        "}\n"
      ],
      "metadata": {
        "id": "bpox5rRbBPwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " best_valid_loss = float('inf')\n",
        "\n",
        "# Concat data\n",
        "data = languages(settings['training_language'])\n",
        "\n",
        "# Choose Language Model\n",
        "lang_model = language_model(settings['language_model'])\n",
        "\n",
        "# Choose model\n",
        "cmodel,ctokenizer = choose_model(settings['model'])\n",
        "\n",
        "# Train Test split\n",
        "train_dataset,val_dataset = train_test_split(data,test_size=0.15,random_state=21)\n",
        "  \n",
        "# Transfom Data\n",
        "transform = Transform_Data(train_dataset,val_dataset,lang_model,'text','label',ctokenizer)\n",
        "train_encodings, train_y, val_encodings,val_y, train_dataloader, val_dataloader = transform.Execute()\n",
        "\n",
        "# Initiate Model\n",
        "\n",
        "model_arch = cmodel.from_pretrained(lang_model)\n",
        "model = Model_Arch(model_arch,False)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Train Model\n",
        "language = settings['training_language']\n",
        "language = '_'.join(language)\n",
        "lanmodel = settings['model']\n",
        "\n",
        "train(model, settings['num_epochs'], train_dataloader,val_dataloader,best_valid_loss,language,lanmodel)"
      ],
      "metadata": {
        "id": "Ya0ex_ZWByli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, test_dataloader):\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    # File in drive\n",
        "    path = '/content/drive/My Drive/Datasets/All/'+lanmodel+'_'+language+'_weights.pt'\n",
        "    model.load_state_dict(torch.load(path))\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "            #preds = logits.detach().cpu().numpy()\n",
        "            \n",
        "        all_logits.append(torch.round(logits))\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "    preds = all_logits.detach().cpu().numpy()\n",
        "    preds = np.concatenate(preds).astype(int).ravel().tolist()\n",
        "    \n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    #probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return preds"
      ],
      "metadata": {
        "id": "k0oi6DvHB0hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_language(clanguage):\n",
        "\n",
        "  if clanguage=='Slovenian' or clanguage=='slovenian':\n",
        "    # Test path Slovenian\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Slovenian/Slovene_test_set.csv\"\n",
        "    \n",
        "    # Clean Slovenian data\n",
        "    slovenian = Slovenian(text_path)\n",
        "    test_data = slovenian.clean_data()\n",
        "\n",
        "  elif clanguage=='Polish' or clanguage=='polish':\n",
        "    # Text path Polish\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Polish/test_set_clean_only_text.txt\"\n",
        "    tag_path = \"/content/drive/My Drive/Datasets/Polish/test_set_clean_only_tags.txt\"\n",
        "    \n",
        "    # Clean Polish data\n",
        "    polish = Polish(text_path,tag_path)\n",
        "    test_data = polish.clean_data()\n",
        "\n",
        "  elif clanguage=='Croatian' or clanguage=='croatian':\n",
        "    # Text path Croatian\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Croatian/Croatian_test_set.csv\"\n",
        "    \n",
        "    # Clean Croatian Data\n",
        "    croatian = Croatian(text_path)\n",
        "    test_data = croatian.clean_data()\n",
        "  \n",
        "  elif clanguage=='Greek' or clanguage=='greek':\n",
        "    # Text path Greek\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Greek/offenseval-gr-test.csv\"\n",
        "    \n",
        "    # Clean data\n",
        "    greek = Greek(text_path)\n",
        "    test_data = greek.clean_data()\n",
        "\n",
        "  return test_data"
      ],
      "metadata": {
        "id": "oUOEOFAEB1JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Polish():\n",
        "\n",
        "    def __init__(self,data_path,tag_path):\n",
        "        self.path = data_path\n",
        "        self.tag_path = tag_path\n",
        "        self.data = self.read_data()\n",
        "        self.tag = self.read_tag()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "\n",
        "        open_data = open(self.path, \"r\", encoding=\"utf8\")\n",
        "\n",
        "        return pd.DataFrame(open_data)\n",
        "\n",
        "    def read_tag(self):\n",
        "\n",
        "        open_tag = open(self.tag_path,'r')\n",
        "\n",
        "        return pd.DataFrame(open_tag)\n",
        "\n",
        "    def remove_punctuation(self,data,column):\n",
        "      \n",
        "      return data[column].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\n",
        "  \n",
        "    def lower(self):\n",
        "\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def rename_columns(self,data,column):\n",
        "\n",
        "        return data.rename(columns={0:column})\n",
        "\n",
        "    def remove_url(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'http\\S+', '',x))\n",
        "\n",
        "    def remove_end_line(self,data,column):\n",
        "\n",
        "        return data[column].str.replace('\\n','')\n",
        "\n",
        "    def concat(self):\n",
        "\n",
        "        self.data[self.label] = self.tag[self.label]\n",
        "\n",
        "        return self.data\n",
        "    \n",
        "    def convert_int(self):\n",
        "\n",
        "      return self.tag[self.label].apply(lambda x: int(x))\n",
        "\n",
        "    def double_space(self):\n",
        "      self.data[self.column].apply(lambda x: x.replace('  ',' '))\n",
        "\n",
        "\n",
        "    def clean_data(self):\n",
        "\n",
        "        text_column = self.column\n",
        "        label_column = self.label\n",
        "\n",
        "        # Rename columns in both label and text data\n",
        "        self.data = self.rename_columns(self.data,text_column)\n",
        "\n",
        "        self.tag = self.rename_columns(self.tag, label_column)\n",
        "\n",
        "        # Remove url\n",
        "        self.remove_url()\n",
        "\n",
        "        # Remove end line character from label and text data\n",
        "        self.data[text_column] = self.remove_end_line(self.data,text_column)\n",
        "\n",
        "        self.tag[label_column] = self.remove_end_line(self.tag,label_column)\n",
        "\n",
        "\n",
        "        # Remove Punctuation\n",
        "        self.data[text_column] = self.remove_punctuation(self.data,text_column) \n",
        "\n",
        "        # Lower words in text data\n",
        "        self.data[text_column] = self.lower()\n",
        "\n",
        "        # Remove double space\n",
        "        self.double_space()\n",
        "\n",
        "        # Convert label to int\n",
        "        self.tag[label_column] = self.convert_int()\n",
        "        \n",
        "        # Concat text and labels\n",
        "\n",
        "        return self.concat()"
      ],
      "metadata": {
        "id": "a8Xjtqbda3tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "class Greek():\n",
        "    def __init__(self,path):\n",
        "        self.path  = path\n",
        "        self.data = self.read_data()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "        return pd.read_csv(self.path)\n",
        "\n",
        "    def rename_columns(self):\n",
        "\n",
        "        return self.data.rename(columns={'tweet':self.column,'subtask_a':self.label})\n",
        "\n",
        "    def replace_hashtag(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(\"#[\\w]+\", \"hashtag \", x))\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: x.replace('hashtag',''))\n",
        "        return self.data[self.column]\n",
        "    \n",
        "    def remove_url(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'http\\S+', '',x))\n",
        "\n",
        "    def strip_punctuation(self):\n",
        "\n",
        "        return self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\n",
        "\n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def remove_stopwords(self, data, column):\n",
        "        data[column] = data[column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"greek\")])\n",
        "        return data[column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 0 if x=='NOT' else 1)\n",
        "    \n",
        "    def double_space(self):\n",
        "      self.data[self.column].apply(lambda x: x.replace('  ',' '))\n",
        "\n",
        "    def clean_data(self):\n",
        "        text_column = self.column\n",
        "        label_column = self.label\n",
        "\n",
        "        # Rename Columns\n",
        "        self.data = self.rename_columns()\n",
        "\n",
        "        # Replace hashtag\n",
        "        self.data[text_column] = self.replace_hashtag()\n",
        "\n",
        "        # Remove urls\n",
        "        self.remove_url()\n",
        "\n",
        "        # Strip Punctuation\n",
        "        self.data[text_column] = self.strip_punctuation()\n",
        "\n",
        "        # Lower text\n",
        "        self.data[text_column] = self.lower()\n",
        "\n",
        "        # Remove Stopwords\n",
        "        self.data[text_column] = self.remove_stopwords(self.data,text_column)\n",
        "\n",
        "        # Remove double space\n",
        "        self.double_space()\n",
        "\n",
        "        # Binarize labels\n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "metadata": {
        "id": "FXkEtpESUh0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import torch, gc\n",
        "\n",
        "\n",
        "# Choose Testing dataset\n",
        "test_data = choose_language('polish')\n",
        "\n",
        "lang_model = language_model(settings['language_model'])\n",
        "cmodel,ctokenizer = choose_model(settings['model'])\n",
        "\n",
        "# Tokenize and Encode Data\n",
        "tokenizer = ctokenizer.from_pretrained(lang_model)\n",
        "test_encodings = tokenizer.batch_encode_plus(test_data['text'].tolist(),add_special_tokens = True, truncation=True, padding=True, max_length=60,return_tensors='pt')\n",
        "\n",
        "# Convert input_ids and attention_mask to tensors\n",
        "test_seq = torch.tensor(test_encodings['input_ids'])\n",
        "test_mask = torch.tensor(test_encodings['attention_mask'])\n",
        "\n",
        "# Sample data\n",
        "test_dataset = TensorDataset(test_seq, test_mask)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n",
        "\n",
        "test_y = torch.tensor(test_data['label'].tolist())\n",
        "\n",
        "# Model\n",
        "bert = cmodel.from_pretrained(lang_model)\n",
        "model = Model_Arch(bert,False)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Predict\n",
        "test_pred = predict(model, test_dataloader)\n",
        "test_label = test_y.detach().cpu().numpy()\n",
        "\n",
        "# Print Results\n",
        "print(test_label)\n",
        "print(len(test_label))\n",
        "print(len(test_pred))\n",
        "print(classification_report(test_label,test_pred))\n"
      ],
      "metadata": {
        "id": "_VrnAvICB25A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fKmNtWvQUXhW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}