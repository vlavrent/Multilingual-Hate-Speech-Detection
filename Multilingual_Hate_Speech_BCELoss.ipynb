{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multilingual Hate Speech_BCELoss.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNHkgikKrIYWlPf/XJPktEq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b125fe3305c4fd5986d1ef3e1782079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fad6a89a322048e39ec258be93c94dce",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e2a7b2ad7bbe444097168fa7cd30dd52",
              "IPY_MODEL_3d43520e4ff0401c965ccd2f3db2bbb1",
              "IPY_MODEL_58b655f6339a45fa9f8a54e9e677ab8d"
            ]
          }
        },
        "fad6a89a322048e39ec258be93c94dce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2a7b2ad7bbe444097168fa7cd30dd52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cc3711d0fc344a41bde2ec29c1fb93c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2de710927c244bfda0676aef19288ef2"
          }
        },
        "3d43520e4ff0401c965ccd2f3db2bbb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_941c80c731ad45919551c4b062820d71",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 585,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 585,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cfc1295ea4e04c219b48c2fa58477a30"
          }
        },
        "58b655f6339a45fa9f8a54e9e677ab8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d823ce83b04545d697afd37664fdaeab",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 585/585 [12:07&lt;00:00,  1.10it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad03ed2eebfd472f982de3a24c90e6f1"
          }
        },
        "cc3711d0fc344a41bde2ec29c1fb93c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2de710927c244bfda0676aef19288ef2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "941c80c731ad45919551c4b062820d71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cfc1295ea4e04c219b48c2fa58477a30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d823ce83b04545d697afd37664fdaeab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad03ed2eebfd472f982de3a24c90e6f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlavrent/Multilingual-Hate-Speech-Detection/blob/main/Multilingual_Hate_Speech_BCELoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbnzyy25P6bA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139d60ec-55dd-4f60-c38c-471bade5dc17"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.14.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3Jvc37Trg3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f7bd70-3f60-44e7-9c76-3aaadd6b8b29"
      },
      "source": [
        "pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW98BXjgQhrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e624aef0-c9ba-452c-8bcd-2079d8bcbf6b"
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiYEUcDtQil-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "191575af-cd1a-4968-e5d8-0cea28c1cfa7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOA5DdeYQng2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c75326a-32f9-4dce-a6b0-c2be6beda9b7"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "  print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7aJETliQprv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46bfd1bf-3bd8-42ef-da20-b3fe367fb2ff"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#from torchsampler import ImbalancedDatasetSampler\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "from sklearn.model_selection import KFold\n",
        "import torch, gc\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from sklearn.metrics import f1_score,classification_report\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22zIRNZwnr_O"
      },
      "source": [
        "<h1>Preprocess Polish</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOviFAICQsD0"
      },
      "source": [
        "class Polish():\n",
        "\n",
        "    def __init__(self,data_path,tag_path):\n",
        "        self.path = data_path\n",
        "        self.tag_path = tag_path\n",
        "        self.data = self.read_data()\n",
        "        self.tag = self.read_tag()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "\n",
        "        open_data = open(self.path, \"r\", encoding=\"utf8\")\n",
        "\n",
        "        return pd.DataFrame(open_data)\n",
        "\n",
        "    def read_tag(self):\n",
        "\n",
        "        open_tag = open(self.tag_path,'r')\n",
        "\n",
        "        return pd.DataFrame(open_tag)\n",
        "\n",
        "    def remove_punctuation(self,data,column):\n",
        "      \n",
        "      return data[column].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\n",
        "  \n",
        "    def lower(self):\n",
        "\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def rename_columns(self,data,column):\n",
        "\n",
        "        return data.rename(columns={0:column})\n",
        "\n",
        "    def remove_mentions(self):\n",
        "\n",
        "        return self.data[self.column].apply(lambda row: re.sub(\"@[A-Za-z0-9]+_[A-Za-z0-9]+\",\"\",row))\n",
        "\n",
        "    def remove_end_line(self,data,column):\n",
        "\n",
        "        return data[column].str.replace('\\n','')\n",
        "\n",
        "    def concat(self):\n",
        "\n",
        "        self.data[self.label] = self.tag[self.label]\n",
        "\n",
        "        return self.data\n",
        "    \n",
        "    def convert_int(self):\n",
        "\n",
        "      return self.tag[self.label].apply(lambda x: int(x))\n",
        "\n",
        "\n",
        "    def clean_data(self):\n",
        "\n",
        "        text_column = self.column\n",
        "        label_column = self.label\n",
        "\n",
        "        # Rename columns in both label and text data\n",
        "        self.data = self.rename_columns(self.data,text_column)\n",
        "\n",
        "        self.tag = self.rename_columns(self.tag, label_column)\n",
        "\n",
        "        # Remove Punctuation\n",
        "        self.data[text_column] = self.remove_punctuation(self.data,text_column) \n",
        "\n",
        "        # Lower words in text data\n",
        "        self.data[text_column] = self.lower()\n",
        "\n",
        "        # Remove user mentions in text data\n",
        "        self.data[text_column] = self.remove_mentions()\n",
        "\n",
        "        # Remove end line character from label and text data\n",
        "        self.data[text_column] = self.remove_end_line(self.data,text_column)\n",
        "\n",
        "        self.tag[label_column] = self.remove_end_line(self.tag,label_column)\n",
        "\n",
        "        # Convert label to int\n",
        "        self.tag[label_column] = self.convert_int()\n",
        "        \n",
        "\n",
        "        # Concat text and labels\n",
        "\n",
        "        return self.concat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMjctOUWn7RQ"
      },
      "source": [
        "<h1>Preprocess Slovenian</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3Z5Cj8SQt7p"
      },
      "source": [
        "class Slovenian():\n",
        "    def __init__(self,path):\n",
        "        self.path = path\n",
        "        self.data = self.read_data()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "        return pd.read_csv(self.path)\n",
        "\n",
        "    def rename_columns(self):\n",
        "\n",
        "        self.data = self.data[['text','type']]\n",
        "        return self.data.rename(columns={'type':self.label})\n",
        "\n",
        "    def strip_punctuation(self):\n",
        "        return self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
        "\n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def remove_stopwords(self, data, column):\n",
        "        data[column] = data[column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"slovene\")])\n",
        "        return data[column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def change_labels(self,x):\n",
        "\n",
        "        change = {'Background offensive':'offensive', 'Acceptable speech':'not offensive', 'Background violence':'violence',\n",
        "                  'Other offensive':'offensive', 'Inappropriate':'innappropriate', 'Other violence':'violence'}\n",
        "\n",
        "        for k, v in change.items():\n",
        "\n",
        "             x = x.replace(k, v)\n",
        "        return x\n",
        "\n",
        "    def convert_labels(self):\n",
        "\n",
        "        self.data[self.label] = self.data[self.label].apply(lambda x: self.change_labels(x))\n",
        "\n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 1 if x=='offensive' else 0)\n",
        "\n",
        "\n",
        "    def clean_data(self):\n",
        "\n",
        "        # Rename Columns\n",
        "        self.data = self.rename_columns()\n",
        "\n",
        "        # Strip Punctuation\n",
        "        self.data[self.column] = self.strip_punctuation()\n",
        "\n",
        "        # Lowercase\n",
        "        self.data[self.column] = self.lower()\n",
        "\n",
        "        # Remove stopwords\n",
        "        self.data[self.column] = self.remove_stopwords(self.data,self.column)\n",
        "        \n",
        "        # Convert labels\n",
        "        self.convert_labels()\n",
        "        \n",
        "        # Binarize labels\n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvPd-R_5n_Pg"
      },
      "source": [
        "<h1>Preprocess Croatian</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxh_Z4cHQwxP"
      },
      "source": [
        "class Croatian():\n",
        "    def __init__(self,path):\n",
        "        self.path = path\n",
        "        self.data = self.read_data()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "        return pd.read_csv(self.path)\n",
        "\n",
        "    def rename_columns(self):\n",
        "\n",
        "        self.data = self.data[['text','type']]\n",
        "        return self.data.rename(columns={'type':self.label})\n",
        "\n",
        "    def strip_punctuation(self):\n",
        "        return self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
        "\n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def remove_stopwords(self, data, column):\n",
        "        data[column] = data[column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"slovene\")])\n",
        "        return data[column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def change_labels(self,x):\n",
        "\n",
        "        change = {'Background offensive':'offensive', 'Acceptable speech':'not offensive', 'Background violence':'offensive',\n",
        "                  'Other offensive':'offensive', 'Inappropriate':'offensive', 'Other violence':'offensive'}\n",
        "\n",
        "        for k, v in change.items():\n",
        "\n",
        "             x = x.replace(k, v)\n",
        "        return x\n",
        "\n",
        "    def convert_labels(self):\n",
        "\n",
        "        self.data[self.label] = self.data[self.label].apply(lambda x: self.change_labels(x))\n",
        "\n",
        "        hate = ['offensive','not offensive']\n",
        "\n",
        "        self.data = self.data[self.data[self.label].isin(hate)]\n",
        "    \n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 1 if x=='offensive' else 0)\n",
        "\n",
        "\n",
        "    def clean_data(self):\n",
        "\n",
        "        # Rename Columns\n",
        "        self.data = self.rename_columns()\n",
        "\n",
        "        # Strip Punctuation\n",
        "        self.data[self.column] = self.strip_punctuation()\n",
        "\n",
        "        # Lowercase\n",
        "        self.data[self.column] = self.lower()\n",
        "\n",
        "        # Remove stopwords\n",
        "        self.data[self.column] = self.remove_stopwords(self.data,self.column)\n",
        "\n",
        "        self.convert_labels()\n",
        "\n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0-dD8ZBoFHh"
      },
      "source": [
        "<h1>Preprocess Greek</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGmn_vzx3a4X"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "class Greek():\n",
        "    def __init__(self,path):\n",
        "        self.path  = path\n",
        "        self.data = self.read_data()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "        return pd.read_csv(self.path)\n",
        "\n",
        "    def rename_columns(self):\n",
        "\n",
        "        return self.data.rename(columns={'tweet':self.column,'subtask_a':self.label})\n",
        "\n",
        "    def replace_hashtag(self):\n",
        "\n",
        "        return self.data[self.column].apply(lambda x: re.sub(\"#[\\w]+\",\"hashtag\",x))\n",
        "\n",
        "    def strip_punctuation(self):\n",
        "\n",
        "        return self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\n",
        "\n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def remove_stopwords(self, data, column):\n",
        "        data[column] = data[column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"greek\")])\n",
        "        return data[column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 0 if x=='NOT' else 1)\n",
        "\n",
        "    def clean_data(self):\n",
        "        text_column = self.column\n",
        "        label_column = self.label\n",
        "\n",
        "        # Rename Columns\n",
        "        self.data = self.rename_columns()\n",
        "\n",
        "        # Replace hashtag\n",
        "        #self.data[text_column] = self.replace_hashtag()\n",
        "\n",
        "        # Strip Punctuation\n",
        "        self.data[text_column] = self.strip_punctuation()\n",
        "\n",
        "        # Lower text\n",
        "        self.data[text_column] = self.lower()\n",
        "\n",
        "        # Remove Stopwords\n",
        "        #self.data[text_column] = self.remove_stopwords(self.data,text_column)\n",
        "\n",
        "        # Binarize labels\n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUSbFr7LoYtM"
      },
      "source": [
        "<h1>Preprocess English</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1bH83t53oOY"
      },
      "source": [
        "class English():\n",
        "\n",
        "    def __init__(self,path):\n",
        "        self.data = self.read(path)\n",
        "        self.label = 'label'\n",
        "        self.column = 'text'\n",
        "\n",
        "    def read(self,path):\n",
        "        return pd.read_csv(path)\n",
        "\n",
        "    def replace_label(self,x):\n",
        "        if ('normal' in x) or (x==2):\n",
        "            return 'NOT'\n",
        "        else:\n",
        "            return 'HOF'\n",
        "\n",
        "    def fix_label(self):\n",
        "        self.data[self.label] = self.data[self.label].apply(lambda x: self.replace_label(x))\n",
        "\n",
        "\n",
        "    def replace_mentions(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda row: re.sub(\"@[A-Za-z0-9]+_*[A-Za-z0-9]+\", \"mention\", row))\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda row: re.sub(\"mention_\", \"mention\", row))\n",
        "\n",
        "    def remove_punctuation(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "    def replace_hashtag(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(\"#[\\w]+\", \"hashtag\", x))\n",
        "\n",
        "    def remove_stopwords(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"english\")])\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def remove_url(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '',x))\n",
        "    \n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 0 if x=='NOT' else 1)\n",
        "\n",
        "    def clean_data(self):\n",
        "        # Fix labels\n",
        "        self.fix_label()\n",
        "\n",
        "        # Remove urls\n",
        "        self.remove_url()\n",
        "\n",
        "        # Replace mentions\n",
        "        self.replace_mentions()\n",
        "\n",
        "        # Replace hashtags\n",
        "        self.replace_hashtag()\n",
        "\n",
        "        # Remove punctuation\n",
        "        self.remove_punctuation()\n",
        "\n",
        "        # Remove stopwords\n",
        "        self.remove_stopwords()\n",
        "\n",
        "        # Lower text\n",
        "        self.lower()\n",
        "\n",
        "        # Binarize labels\n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbmhAi42oho8"
      },
      "source": [
        "<h1>Tokenizer, Tensors and Dataloaders</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTNquqlzQy41"
      },
      "source": [
        "class Transform_Data():\n",
        "  def __init__(self,train,test,language_model,column,label,cmodel):\n",
        "    self.max_length = 60\n",
        "    self.tokenizer = cmodel.from_pretrained(language_model)\n",
        "    self.train = train\n",
        "    self.test = test\n",
        "    self.column = column\n",
        "    self.label = label\n",
        "\n",
        "  def Tokenizer(self):\n",
        "\n",
        "    train_encodings = self.tokenizer.batch_encode_plus(self.train[self.column].tolist(),add_special_tokens = True, truncation=True, padding=True, max_length=self.max_length,return_tensors='pt')\n",
        "    train_y = torch.tensor(self.train[self.label].tolist())\n",
        "  \n",
        "    val_encodings = self.tokenizer.batch_encode_plus(self.test[self.column].tolist(),add_special_tokens = True, truncation=True, padding=True, max_length=self.max_length,return_tensors='pt')\n",
        "    val_y = torch.tensor(self.test[self.label].tolist())\n",
        "\n",
        "    return train_encodings, train_y, val_encodings,val_y \n",
        "\n",
        "  def Tensors_and_DataLoaders(self,train_encodings,train_y,val_encodings, val_y):\n",
        "\n",
        "    #====================\n",
        "    #Train data\n",
        "    #====================\n",
        "    train_data = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_y)\n",
        "    class_samples = [(train_y == 0.).sum(dim=0),(train_y == 1.).sum(dim=0)]\n",
        "    total_samples = sum(class_samples)\n",
        "    \n",
        "\n",
        "    class_weights = [total_samples/class_samples[i] for i in range(len(class_samples))]\n",
        "    weights = [class_weights[train_y[i]] for i in range(int(total_samples))]\n",
        "\n",
        "    train_sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(total_samples))\n",
        "    \n",
        "\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=64)\n",
        "\n",
        "\n",
        "    #====================\n",
        "    #Test/Validation data\n",
        "    #====================\n",
        "\n",
        "    val_data = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_y)\n",
        "\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=64)\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "  def Execute(self):\n",
        "\n",
        "    train_encodings, train_y, val_encodings,val_y = self.Tokenizer()\n",
        "    train_dataloader, val_dataloader = self.Tensors_and_DataLoaders(train_encodings,train_y,val_encodings, val_y)\n",
        "    return train_encodings, train_y, val_encodings,val_y, train_dataloader, val_dataloader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xN1Gnvqolgc"
      },
      "source": [
        "<h1>Output Layer</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGFcA3kAQ58x"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Model_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, ROBERTA,freeze_bert):\n",
        "      \n",
        "      super(Model_Arch, self).__init__()\n",
        "\n",
        "      self.bert = ROBERTA\n",
        "      self.freeze_bert = freeze_bert\n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.3)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(self.bert.config.hidden_size,1)\n",
        "      \n",
        "\n",
        "      # sigmoid activation function\n",
        "      self.sigmoid =  nn.Sigmoid()\n",
        "      #self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "      if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _,cls_hs = self.bert(sent_id, attention_mask=mask,return_dict=False) #,return_dict=False\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "      \n",
        "      # activation function\n",
        "      #x = self.relu(x)\n",
        "      \n",
        "      # dropout\n",
        "      #x = self.dropout(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.sigmoid(x)\n",
        "      #x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffWF0qTKo3EN"
      },
      "source": [
        "<h1>Optimizer, Scheduler</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NgaShAKRFA1"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import get_scheduler\n",
        "\n",
        "\n",
        "def Optimizer_Scheduler(model,train_dataloader,num_epochs):\n",
        "  \n",
        "  optimizer = AdamW(model.parameters(), lr=1e-5) #weight_decay=0.02 ,eps=1e-8\n",
        " \n",
        "  num_training_steps = num_epochs * len(train_dataloader)\n",
        "  lr_scheduler = get_linear_schedule_with_warmup(\n",
        "                       optimizer=optimizer,\n",
        "                       num_warmup_steps=0,\n",
        "                       num_training_steps=num_training_steps)\n",
        "\n",
        "  return optimizer, lr_scheduler, num_training_steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr7ha0keo9v_"
      },
      "source": [
        "<h1>Train Model</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOj2q8iNRG7h"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def Mean(data):\n",
        "    return sum(data) / len(data)\n",
        "\n",
        "def train(model, num_epochs, train_dataloader,val_dataloader,best_f1,language,lanmodel):\n",
        "\n",
        "  # Optimizer and Scheduler\n",
        "  optimizer, lr_scheduler, num_training_steps = Optimizer_Scheduler(model,train_dataloader,num_epochs)\n",
        "\n",
        "  progress_bar = tqdm(range(num_training_steps))\n",
        "  loss_fn = nn.BCELoss()\n",
        "  #loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "  # Initialize arrays\n",
        "  train_acc = []\n",
        "  val_acc = [] \n",
        "  train_loss = []  \n",
        "  val_loss = []\n",
        "  avg_acc_0 = []\n",
        "  avg_acc_1 = []\n",
        "  total_avg_loss_train = []\n",
        "  total_avg_loss_val = []\n",
        "\n",
        "  # Set a flag for results\n",
        "  flag = True\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "    \n",
        "    total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "    predictions = []\n",
        "    real_label = []\n",
        "\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      batch_counts += 1\n",
        "      \n",
        "      # batch to GPU\n",
        "      batch = [r.to(device) for r in batch]\n",
        "\n",
        "      sent_id, mask, labels = batch\n",
        "      real_label.append(labels.detach().cpu().numpy())\n",
        "      \n",
        "      \n",
        "      # clear previous gradients\n",
        "      model.zero_grad()\n",
        "       \n",
        "      # predictions for current batch\n",
        "      output = model(sent_id, mask)\n",
        "\n",
        "      pred = torch.round(output)\n",
        "      #pred = torch.argmax(output,1)\n",
        "\n",
        "      pred = pred.detach().cpu().numpy()\n",
        "      pred = pred.flatten()\n",
        "      predictions.append(pred)\n",
        "\n",
        "      \n",
        "      \n",
        "      # compute loss for current batch\n",
        "      loss = loss_fn(output, labels.unsqueeze(1).float())\n",
        "      #loss = loss_fn(output, labels)\n",
        "\n",
        "      # add total loss\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "\n",
        "      # backpropagation to calculate gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # prevent the exploding gradient problem\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "\n",
        "      # update parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      # update scheduler \n",
        "      lr_scheduler.step()\n",
        "\n",
        "      # clear optimizer gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      progress_bar.update(1)\n",
        "\n",
        "    # compute training loss of each batch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # flatten labels and predictions\n",
        "    flat_label = np.concatenate(real_label).astype(int).ravel().tolist()\n",
        "    \n",
        "    flat_predictions = np.concatenate(predictions).astype(int).ravel().tolist()\n",
        "\n",
        "    # Append accuracy and validation loss for training data\n",
        "    train_acc.append(f1_score(flat_label,flat_predictions))\n",
        "    train_loss.append(avg_loss)\n",
        "\n",
        "    # Validation\n",
        "    val_flat_label,val_flat_predictions, val_avg_loss = validate(model,val_dataloader,loss_fn)\n",
        "\n",
        "    if f1_score(val_flat_label,val_flat_predictions) > best_f1:\n",
        "        best_f1 = f1_score(val_flat_label,val_flat_predictions)\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/Datasets/All/'+lanmodel+'_'+language+'_weights.pt')     \n",
        "    \n",
        "    \n",
        "    # Append accuracy and validation loss for validation data\n",
        "    val_acc.append(f1_score(val_flat_label,val_flat_predictions))\n",
        "    val_loss.append(val_avg_loss)\n",
        "\n",
        "    # Compute each class Accuracy (validation set)\n",
        "    acc_0,acc_1 = Class_F1_score(val_flat_label,val_flat_predictions)\n",
        "    avg_acc_0.append(acc_0)\n",
        "    avg_acc_1.append(acc_1)\n",
        "\n",
        "    # Print table with insights\n",
        "    if flag:\n",
        "      print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Train F1-score':^9} | {'Val F1-score':^9} | {'Class_0 F1-score':^10} | {'Class_1 F1-score':^10} \")\n",
        "      flag = False\n",
        "\n",
        "    print(\"-\"*85)\n",
        "    print(f\"{str(epoch + 1) +'/'+ str(num_epochs):^7} | {avg_loss:^12.6f} | {val_avg_loss:^10.6f} | {f1_score(flat_label,flat_predictions):^9.2f} | {f1_score(val_flat_label,val_flat_predictions):^9.2f} | {acc_0:^10.2f} | {acc_1:^10.2f} \")\n",
        "  \n",
        "  \n",
        "\n",
        "  # Print mean of Accuracy and Loss\n",
        "  print(\"-\"*85)\n",
        "  print(f\"{'Average':^7} | {Mean(train_loss):^12.6f} | {Mean(val_loss):^10.6f} | {Mean(train_acc):^9.2f} | {Mean(val_acc):^9.2f} | {Mean(avg_acc_0):^10.2f} | {Mean(avg_acc_1):^10.2f} \")\n",
        "\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhFSbjS1pCaP"
      },
      "source": [
        "<h1>Validate Model</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpM2TODNRJuH"
      },
      "source": [
        "def validate(model,val_dataloader,loss_fn):\n",
        "  \n",
        "    \n",
        "\n",
        "    model.eval() \n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    \n",
        "    val_preds = []\n",
        "    val_label = []\n",
        "    \n",
        "    for step,batch in enumerate(val_dataloader):\n",
        "      \n",
        "      batch = [t.to(device) for t in batch]\n",
        "      \n",
        "      sent_id, mask, labels = batch\n",
        "      val_label.append(labels.detach().cpu().numpy())\n",
        "      \n",
        "      with torch.no_grad():\n",
        "        output = model(sent_id,mask)\n",
        "        \n",
        "      loss = loss_fn(output,labels.unsqueeze(1).float())  \n",
        "      #loss = loss_fn(output,labels) \n",
        "      total_loss = total_loss + loss.item()\n",
        "        \n",
        "        \n",
        "      pred = torch.round(output)\n",
        "      #pred = torch.argmax(output,1)\n",
        "      pred = pred.detach().cpu().numpy()\n",
        "      pred = pred.flatten()\n",
        "      val_preds.append(pred)\n",
        "\n",
        "    # compute training loss of each epoch\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "    print('Total Loss: '+str(total_loss))\n",
        "    print('Avg Loss: '+str(avg_loss))\n",
        "\n",
        "    # flatten labels and predictions\n",
        "    flat_label = np.concatenate(val_label).astype(int).ravel().tolist()\n",
        "    \n",
        "    \n",
        "    flat_predictions = np.concatenate(val_preds).astype(int).ravel().tolist()\n",
        "\n",
        "    \n",
        "\n",
        "    return flat_label,flat_predictions, avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-rwlDQMpGHR"
      },
      "source": [
        "<h1>F1 Score per Class</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QXwxOERRLqc"
      },
      "source": [
        "def Class_F1_score(val_y,new_preds):\n",
        "\n",
        "  report = classification_report(val_y,new_preds, output_dict=True )\n",
        "\n",
        "  return report['0']['f1-score'], report['1']['f1-score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T21KkGspP4f"
      },
      "source": [
        "<h1>Balancing with kmedoid</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcKtne5hPy1U"
      },
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "import numpy as np\n",
        "import random\n",
        "from past.builtins import xrange\n",
        "\n",
        "def kMedoids(D, k, tmax=100):\n",
        "    # determine dimensions of distance matrix D\n",
        "    m, n = D.shape\n",
        "\n",
        "    if k > n:\n",
        "        raise Exception('too many medoids')\n",
        "\n",
        "    # find a set of valid initial cluster medoid indices since we\n",
        "    # can't seed different clusters with two points at the same location\n",
        "    valid_medoid_inds = set(range(n))\n",
        "    invalid_medoid_inds = set([])\n",
        "    rs,cs = np.where(D==0)\n",
        "    # the rows, cols must be shuffled because we will keep the first duplicate below\n",
        "    index_shuf = list(range(len(rs)))\n",
        "    np.random.shuffle(index_shuf)\n",
        "    rs = rs[index_shuf]\n",
        "    cs = cs[index_shuf]\n",
        "    for r,c in zip(rs,cs):\n",
        "        # if there are two points with a distance of 0...\n",
        "        # keep the first one for cluster init\n",
        "        if r < c and r not in invalid_medoid_inds:\n",
        "            invalid_medoid_inds.add(c)\n",
        "    valid_medoid_inds = list(valid_medoid_inds - invalid_medoid_inds)\n",
        "\n",
        "    if k > len(valid_medoid_inds):\n",
        "        raise Exception('too many medoids (after removing {} duplicate points)'.format(\n",
        "            len(invalid_medoid_inds)))\n",
        "\n",
        "    # randomly initialize an array of k medoid indices\n",
        "    M = np.array(valid_medoid_inds)\n",
        "    np.random.shuffle(M)\n",
        "    M = np.sort(M[:k])\n",
        "\n",
        "    # create a copy of the array of medoid indices\n",
        "    Mnew = np.copy(M)\n",
        "\n",
        "    # initialize a dictionary to represent clusters\n",
        "    C = {}\n",
        "    for t in xrange(tmax):\n",
        "        # determine clusters, i. e. arrays of data indices\n",
        "        J = np.argmin(D[:,M], axis=1)\n",
        "        for kappa in range(k):\n",
        "            C[kappa] = np.where(J==kappa)[0]\n",
        "        # update cluster medoids\n",
        "        for kappa in range(k):\n",
        "            J = np.mean(D[np.ix_(C[kappa],C[kappa])],axis=1)\n",
        "            j = np.argmin(J)\n",
        "            Mnew[kappa] = C[kappa][j]\n",
        "        np.sort(Mnew)\n",
        "        # check for convergence\n",
        "        if np.array_equal(M, Mnew):\n",
        "            break\n",
        "        M = np.copy(Mnew)\n",
        "    else:\n",
        "        # final update of cluster memberships\n",
        "        J = np.argmin(D[:,M], axis=1)\n",
        "        for kappa in range(k):\n",
        "            C[kappa] = np.where(J==kappa)[0]\n",
        "\n",
        "    # return results\n",
        "    return M, C\n",
        "\n",
        "def compare(ratio,data,balance):\n",
        "    class_tags = data['label'].value_counts().to_list()\n",
        "    if ratio>=0.5:\n",
        "        total = balance - min(class_tags)\n",
        "\n",
        "        # Find Class to balance\n",
        "        balance_class_index = class_tags.index(max(class_tags))\n",
        "        class_index = data['label'].value_counts().index.to_list()\n",
        "\n",
        "        # Find Class not to balance\n",
        "        balance_class_index_not = class_tags.index(max(class_tags)) \n",
        "        class_index_not = data['label'].value_counts().index.to_list()\n",
        "        \n",
        "        # Choose Class to sample\n",
        "        new = data[data['label']==class_index[balance_class_index]]\n",
        "        print(new)\n",
        "\n",
        "        # Choose class not to sample\n",
        "        data2 = data[data['label']==class_index_not[balance_class_index_not]]\n",
        "        print(data2)\n",
        "      \n",
        "        # Transform data and undersample\n",
        "        model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "        embeddings = model.encode(new['text'].to_numpy())\n",
        "\n",
        "        D = pairwise_distances(embeddings, metric='cosine')\n",
        "\n",
        "        M, C = kMedoids(D, total)\n",
        "        data1 = new.iloc[M]\n",
        "        \n",
        "        \n",
        "        return pd.concat([data1,data2],ignore_index=True)\n",
        "    else:\n",
        "      model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "      embeddings = model.encode(data['text'].to_numpy())\n",
        "\n",
        "      D = pairwise_distances(embeddings, metric='cosine')\n",
        "\n",
        "      M, C = kMedoids(D, balance)\n",
        "      return data.iloc[M]\n",
        "\n",
        "\n",
        "def check_imbalance_ratio(data):\n",
        "    class_tags = data['label'].value_counts().to_list()\n",
        "    difference = max(class_tags) - min(class_tags)\n",
        "    ratio = difference/max(class_tags)\n",
        "    return ratio\n",
        "\n",
        "def balance(data,sample):\n",
        "    return compare(check_imbalance_ratio(data),data,sample)\n",
        "\n",
        "def execute(data,sample):\n",
        "   \n",
        "    return balance(data,sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7QOp-rdpeWJ"
      },
      "source": [
        "<h1>Choose dataset for Training</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F54V2TyR3HA"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def find_length(language):\n",
        "  length = []\n",
        "  \n",
        "  for count, value in enumerate(language):\n",
        "    \n",
        "    if value=='slovenian' or value=='Slovenian':\n",
        "      # Slovenian\n",
        "      text_path = \"/content/drive/My Drive/Datasets/Slovenian/Slovene_train_set.csv\"\n",
        "      length.append(len(pd.read_csv(text_path).index))\n",
        "\n",
        "    elif value=='polish' or value=='Polish':\n",
        "      # Polish\n",
        "      text_path = \"/content/drive/My Drive/Datasets/Polish/training_set_clean_only_text.txt\"\n",
        "      tag_path = \"/content/drive/My Drive/Datasets/Polish/training_set_clean_only_tags.txt\"\n",
        "      length.append(len(pd.read_csv(open(tag_path, \"r\", encoding=\"utf8\")).index))\n",
        "      \n",
        "    elif value=='croatian' or value=='Croatian':\n",
        "      # Croatian\n",
        "      text_path = \"/content/drive/My Drive/Datasets/Croatian/Croatian_train_set.csv\"\n",
        "      length.append(len(pd.read_csv(text_path).index))\n",
        "\n",
        "    elif value=='greek' or value=='Greek':\n",
        "      # Greek\n",
        "      text_path = \"/content/drive/My Drive/Datasets/Greek/offenseval-gr_train.csv\"\n",
        "      \n",
        "      length.append(len(pd.read_csv(text_path).index))\n",
        "  return min(length)\n",
        " \n",
        "\n",
        "\n",
        "def preprocess_language(language,length,number_language):\n",
        "\n",
        "  if language=='slovenian' or language=='Slovenian':\n",
        "    # Slovenian\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Slovenian/Slovene_train_set.csv\"\n",
        "\n",
        "    slovenian = Slovenian(text_path)\n",
        "    data = slovenian.clean_data()\n",
        "\n",
        "  elif language=='polish' or language=='Polish':\n",
        "    # Polish\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Polish/training_set_clean_only_text.txt\"\n",
        "    tag_path = \"/content/drive/My Drive/Datasets/Polish/training_set_clean_only_tags.txt\"\n",
        "\n",
        "    polish = Polish(text_path,tag_path)\n",
        "    data = polish.clean_data()\n",
        "    if number_language>1 and len(data)>length:\n",
        "          data  = execute(data,length)\n",
        "  \n",
        "  elif language=='croatian' or language=='Croatian':\n",
        "    # Croatian\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Croatian/Croatian_train_set.csv\"\n",
        "\n",
        "    croatian = Croatian(text_path)\n",
        "    data = croatian.clean_data()\n",
        "    if number_language>1 and len(data)>length:\n",
        "          data  = execute(data,length)\n",
        "\n",
        "  elif language=='greek' or language=='Greek':\n",
        "    # Greek\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Greek/offenseval-gr_train.csv\"\n",
        "\n",
        "    greek = Greek(text_path)\n",
        "    data = greek.clean_data()\n",
        "    if number_language>1 and len(data)>length:\n",
        "      data  = execute(data,length)\n",
        "  elif language=='english' or language=='English':\n",
        "    # English\n",
        "    text_path = \"/content/drive/My Drive/Datasets/English/English_train_set.csv\"\n",
        "\n",
        "    english = English(text_path)\n",
        "    data = english.clean_data()\n",
        "    if number_language>1 and len(data)>length:\n",
        "      data  = execute(data,length)\n",
        "  return data\n",
        "\n",
        "def languages(lang):\n",
        "  \n",
        "  total_languages = []\n",
        "  length = find_length(lang)\n",
        "  print(len(lang))\n",
        "  \n",
        "\n",
        "  for count, value in enumerate(lang):\n",
        "    \n",
        "    total_languages.append(preprocess_language(value,length,len(lang)))\n",
        "  \n",
        "  return pd.concat(total_languages)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuaLEg1PiUcy"
      },
      "source": [
        "<h1>Choose Model</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rMrGRiMRTe8"
      },
      "source": [
        "def choose_model(cmodel):\n",
        "  if cmodel=='Bert' or cmodel=='BERT' or cmodel=='BERT' or cmodel=='bert':\n",
        "    return BertModel, BertTokenizer\n",
        "  elif cmodel=='ROBERTA' or cmodel=='XLMRoberta' or cmodel=='Roberta' or cmodel=='roberta':\n",
        "    return XLMRobertaModel, XLMRobertaTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H9ChVChiQHA"
      },
      "source": [
        "<h1>Language Model</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEKqYIm9iPon"
      },
      "source": [
        "def language_model(mlang):\n",
        "\n",
        "  if mlang=='multilingual_roberta':\n",
        "    return \"xlm-roberta-base\"\n",
        "\n",
        "  elif mlang=='bert_greek':\n",
        "    return \"nlpaueb/bert-base-greek-uncased-v1\"\n",
        "\n",
        "  elif mlang=='bert_polish':\n",
        "    return \"dkleczek/bert-base-polish-uncased-v1\"\n",
        "  \n",
        "  elif mlang=='bert_croatian':\n",
        "    return \"EMBEDDIA/crosloengual-bert\"\n",
        "  \n",
        "  elif mlang=='bert_slovenian':\n",
        "    return \"EMBEDDIA/sloberta\"\n",
        "  \n",
        "  elif mlang=='multilingual_bert':\n",
        "    return \"bert-base-multilingual-cased\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwWMDhDUiclD"
      },
      "source": [
        "<h1>Model Settings</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeZbQl5-Tk9B"
      },
      "source": [
        "settings = {\n",
        "    'language_model':'multilingual_bert', #multilingual_roberta, multilingual_bert, bert_greek, bert_polish, bert_croatian, bert_slovenian\n",
        "    'model': 'bert',                #roberta, bert\n",
        "    'num_epochs': 5,\n",
        "    'training_language': ['greek','english'] #greek, english, slovenian, polish, croatian\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktU2C51upy6K"
      },
      "source": [
        "<h1>Execute </h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pksc5Y_R6h-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555,
          "referenced_widgets": [
            "0b125fe3305c4fd5986d1ef3e1782079",
            "fad6a89a322048e39ec258be93c94dce",
            "e2a7b2ad7bbe444097168fa7cd30dd52",
            "3d43520e4ff0401c965ccd2f3db2bbb1",
            "58b655f6339a45fa9f8a54e9e677ab8d",
            "cc3711d0fc344a41bde2ec29c1fb93c9",
            "2de710927c244bfda0676aef19288ef2",
            "941c80c731ad45919551c4b062820d71",
            "cfc1295ea4e04c219b48c2fa58477a30",
            "d823ce83b04545d697afd37664fdaeab",
            "ad03ed2eebfd472f982de3a24c90e6f1"
          ]
        },
        "outputId": "47b664c4-fd1f-4827-a1a1-1fd67ce3c041"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "# Concat data\n",
        "data = languages(settings['training_language'])\n",
        "\n",
        "# Choose Language Model\n",
        "lang_model = language_model(settings['language_model'])\n",
        "\n",
        "# Choose model\n",
        "cmodel,ctokenizer = choose_model(settings['model'])\n",
        "\n",
        "# Train Test split\n",
        "train_dataset,val_dataset = train_test_split(data,test_size=0.15,random_state=21)\n",
        "  \n",
        "# Transfom Data\n",
        "transform = Transform_Data(train_dataset,val_dataset,lang_model,'text','label',ctokenizer)\n",
        "train_encodings, train_y, val_encodings,val_y, train_dataloader, val_dataloader = transform.Execute()\n",
        "\n",
        "# Initiate Model\n",
        "\n",
        "model_arch = cmodel.from_pretrained(lang_model)\n",
        "model = Model_Arch(model_arch,False)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Train Model\n",
        "language = settings['training_language']\n",
        "language = '_'.join(language)\n",
        "lanmodel = settings['language_model']\n",
        "print(lanmodel)\n",
        "\n",
        "train(model, settings['num_epochs'], train_dataloader,val_dataloader,0,language,lanmodel)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multilingual_bert\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b125fe3305c4fd5986d1ef3e1782079",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/585 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Loss: 15.0566908121109\n",
            "Avg Loss: 0.7169852767671857\n",
            " Epoch  |  Train Loss  |  Val Loss  | Train F1-score | Val F1-score | Class_0 F1-score | Class_1 F1-score \n",
            "-------------------------------------------------------------------------------------\n",
            "  1/5   |   0.622111   |  0.716985  |   0.66    |   0.54    |    0.64    |    0.54    \n",
            "Total Loss: 10.631460547447205\n",
            "Avg Loss: 0.5062600260689145\n",
            "-------------------------------------------------------------------------------------\n",
            "  2/5   |   0.492592   |  0.506260  |   0.76    |   0.61    |    0.82    |    0.61    \n",
            "Total Loss: 10.794307678937912\n",
            "Avg Loss: 0.5140146513779958\n",
            "-------------------------------------------------------------------------------------\n",
            "  3/5   |   0.423627   |  0.514015  |   0.81    |   0.62    |    0.82    |    0.62    \n",
            "Total Loss: 10.308111160993576\n",
            "Avg Loss: 0.49086243623778936\n",
            "-------------------------------------------------------------------------------------\n",
            "  4/5   |   0.379550   |  0.490862  |   0.84    |   0.63    |    0.84    |    0.63    \n",
            "Total Loss: 10.108517527580261\n",
            "Avg Loss: 0.481357977503822\n",
            "-------------------------------------------------------------------------------------\n",
            "  5/5   |   0.349519   |  0.481358  |   0.85    |   0.65    |    0.86    |    0.65    \n",
            "-------------------------------------------------------------------------------------\n",
            "Average |   0.453480   |  0.541896  |   0.78    |   0.61    |    0.80    |    0.61    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIScla6Up9vl"
      },
      "source": [
        "<h1>Predict class</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irJm2GqZSLre"
      },
      "source": [
        "def predict(model, test_dataloader):\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    # File in drive\n",
        "    path = '/content/drive/My Drive/Datasets/All/'+lanmodel+'_'+language+'_weights.pt'\n",
        "    model.load_state_dict(torch.load(path))\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "            #preds = logits.detach().cpu().numpy()\n",
        "            \n",
        "        all_logits.append(torch.round(logits))\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "    preds = all_logits.detach().cpu().numpy()\n",
        "    preds = np.concatenate(preds).astype(int).ravel().tolist()\n",
        "    \n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    #probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoKHNHjuqTyN"
      },
      "source": [
        "<h1>Choose Language</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gchdhaUgkbDx"
      },
      "source": [
        "def choose_language(clanguage):\n",
        "\n",
        "  if clanguage=='Slovenian' or clanguage=='slovenian':\n",
        "    # Test path Slovenian\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Slovenian/Slovene_test_set.csv\"\n",
        "    \n",
        "    # Clean Slovenian data\n",
        "    slovenian = Slovenian(text_path)\n",
        "    test_data = slovenian.clean_data()\n",
        "\n",
        "  elif clanguage=='Polish' or clanguage=='polish':\n",
        "    # Text path Polish\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Polish/test_set_clean_only_text.txt\"\n",
        "    tag_path = \"/content/drive/My Drive/Datasets/Polish/test_set_clean_only_tags.txt\"\n",
        "    \n",
        "    # Clean Polish data\n",
        "    polish = Polish(text_path,tag_path)\n",
        "    test_data = polish.clean_data()\n",
        "\n",
        "  elif clanguage=='Croatian' or clanguage=='croatian':\n",
        "    # Text path Croatian\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Croatian/Croatian_test_set.csv\"\n",
        "    \n",
        "    # Clean Croatian Data\n",
        "    croatian = Croatian(text_path)\n",
        "    test_data = croatian.clean_data()\n",
        "  \n",
        "  elif clanguage=='Greek' or clanguage=='greek':\n",
        "    # Text path Greek\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Greek/offenseval-gr-test.csv\"\n",
        "    \n",
        "    # Clean data\n",
        "    greek = Greek(text_path)\n",
        "    test_data = greek.clean_data()\n",
        "\n",
        "  return test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suDEh2uNqYEn"
      },
      "source": [
        "<h1>Make Predictions</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r1917d8SMSp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "d77374e1-ac62-4c15-80fe-3eff1fc1315a"
      },
      "source": [
        "import numpy\n",
        "import torch, gc\n",
        "\n",
        "\n",
        "# Choose Testing dataset\n",
        "test_data = choose_language('greek')\n",
        "\n",
        "lang_model = language_model(settings['language_model'])\n",
        "cmodel,ctokenizer = choose_model(settings['model'])\n",
        "\n",
        "# Tokenize and Encode Data\n",
        "tokenizer = ctokenizer.from_pretrained(lang_model)\n",
        "test_encodings = tokenizer.batch_encode_plus(test_data['text'].tolist(),add_special_tokens = True, truncation=True, padding=True, max_length=60,return_tensors='pt')\n",
        "\n",
        "# Convert input_ids and attention_mask to tensors\n",
        "test_seq = torch.tensor(test_encodings['input_ids'])\n",
        "test_mask = torch.tensor(test_encodings['attention_mask'])\n",
        "\n",
        "# Sample data\n",
        "test_dataset = TensorDataset(test_seq, test_mask)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n",
        "\n",
        "test_y = torch.tensor(test_data['label'].tolist())\n",
        "\n",
        "# Model\n",
        "bert = cmodel.from_pretrained(lang_model)\n",
        "model = Model_Arch(bert,False)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Predict\n",
        "test_pred = predict(model, test_dataloader)\n",
        "test_label = test_y.detach().cpu().numpy()\n",
        "\n",
        "# Print Results\n",
        "print(test_label)\n",
        "print(len(test_label))\n",
        "print(len(test_pred))\n",
        "print(classification_report(test_label,test_pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 1 0 0]\n",
            "1544\n",
            "1544\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.80      0.88      1302\n",
            "           1       0.45      0.89      0.60       242\n",
            "\n",
            "    accuracy                           0.81      1544\n",
            "   macro avg       0.71      0.84      0.74      1544\n",
            "weighted avg       0.89      0.81      0.83      1544\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODa11uioE6SE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}