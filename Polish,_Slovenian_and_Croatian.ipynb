{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Polish, Slovenian and Croatian.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMT6YgE1M0bPdEcPMQr939R",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "244ec308dd0846ef8105a9edf8e0a785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3e67725de6054b3db97a2cdcc6919173",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5fc7022818334f38a46512a44324f1ed",
              "IPY_MODEL_01662fd16757448d9276b16c78810adf",
              "IPY_MODEL_15448eb44eda4569a0d61b654be68c64"
            ]
          }
        },
        "3e67725de6054b3db97a2cdcc6919173": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5fc7022818334f38a46512a44324f1ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1aceb5be6f1540f48a1aedbf250e247d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5b6b4ba8f6594e1f88ec62b5791b555b"
          }
        },
        "01662fd16757448d9276b16c78810adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4d63f74ef0c943588ecdfeb5c2e2da13",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 3185,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3185,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd056c4e5c724145ab6cc919b8c00ec6"
          }
        },
        "15448eb44eda4569a0d61b654be68c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4a68711b83e54ccabd185305c3d6611b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3185/3185 [37:32&lt;00:00,  1.59it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f3f7a797682a46a5ac8e553857f2c0e9"
          }
        },
        "1aceb5be6f1540f48a1aedbf250e247d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5b6b4ba8f6594e1f88ec62b5791b555b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d63f74ef0c943588ecdfeb5c2e2da13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd056c4e5c724145ab6cc919b8c00ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a68711b83e54ccabd185305c3d6611b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f3f7a797682a46a5ac8e553857f2c0e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlavrent/Multilingual-Hate-Speech-Detection/blob/main/Polish%2C_Slovenian_and_Croatian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SrxMIlusXSZ"
      },
      "source": [
        "<h1>Polish and Slovenian Hate Speech</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRNmiIUmsU0V",
        "outputId": "1f76d74e-842d-46b2-eb2e-bcb25ec4165e"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUwgcl6POhfk",
        "outputId": "94a2b203-75f5-4475-a0ee-e0387939ddf6"
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXToxL_mZuS8",
        "outputId": "e2d1955b-f49f-4fd8-91f4-974834be7931"
      },
      "source": [
        "pip install sentence_transformers"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.10.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.15.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.96)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.19.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.10.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.0.46)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWoi_n1RsdyA",
        "outputId": "4dc53003-fbdc-4c96-e507-a02851ca2f36"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLhLnChZsgh0",
        "outputId": "bc1d0a2f-d9dc-4d4b-f976-e749a2e873cc"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "  print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl_PMLKSsikq",
        "outputId": "acedc3c6-afb5-4afa-c936-157906712c29"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#from torchsampler import ImbalancedDatasetSampler\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "from sklearn.model_selection import KFold\n",
        "import torch, gc\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from sklearn.metrics import f1_score,classification_report\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw5Q9C0aLJ1Z"
      },
      "source": [
        "<h1>Preprocess English</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kebh3GHKLPld"
      },
      "source": [
        "class English():\n",
        "\n",
        "    def __init__(self,path):\n",
        "        self.data = self.read(path)\n",
        "        self.label = 'label'\n",
        "        self.column = 'text'\n",
        "\n",
        "    def read(self,path):\n",
        "        return pd.read_csv(path)\n",
        "\n",
        "    def replace_label(self,x):\n",
        "        if x=='1' or x=='0' or x=='HOF':\n",
        "            return 'HOF'\n",
        "        else:\n",
        "            return 'NOT'\n",
        "\n",
        "    def fix_label(self):\n",
        "        self.data[self.label] = self.data[self.label].apply(lambda x: self.replace_label(x))\n",
        "\n",
        "\n",
        "    def replace_mentions(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda row: re.sub(\"@[A-Za-z0-9]+_*[A-Za-z0-9]+\", \"mention\", row))\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda row: re.sub(\"mention_\", \"mention\", row))\n",
        "\n",
        "    def remove_punctuation(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "    def replace_hashtag(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(\"#[\\w]+\", \"hashtag\", x))\n",
        "\n",
        "    def remove_stopwords(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"english\")])\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def remove_url(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '',x))\n",
        "    \n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 0 if x=='NOT' else 1)\n",
        "\n",
        "    def clean_data(self):\n",
        "        self.fix_label()\n",
        "        self.remove_url()\n",
        "        self.replace_mentions()\n",
        "        self.replace_hashtag()\n",
        "        self.remove_punctuation()\n",
        "        self.remove_stopwords()\n",
        "        self.lower()\n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "senhQbU5sjiY"
      },
      "source": [
        "<h1>Preprocess Polish</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb5FmVT2spPS"
      },
      "source": [
        "class Polish():\n",
        "\n",
        "    def __init__(self,data_path,tag_path):\n",
        "        self.path = data_path\n",
        "        self.tag_path = tag_path\n",
        "        self.data = self.read_data()\n",
        "        self.tag = self.read_tag()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "\n",
        "        open_data = open(self.path, \"r\", encoding=\"utf8\")\n",
        "\n",
        "        return pd.DataFrame(open_data)\n",
        "\n",
        "    def read_tag(self):\n",
        "\n",
        "        open_tag = open(self.tag_path,'r')\n",
        "\n",
        "        return pd.DataFrame(open_tag)\n",
        "\n",
        "    def remove_punctuation(self,data,column):\n",
        "      \n",
        "      return data[column].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\n",
        "  \n",
        "    def lower(self):\n",
        "\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def rename_columns(self,data,column):\n",
        "\n",
        "        return data.rename(columns={0:column})\n",
        "\n",
        "    def remove_mentions(self):\n",
        "\n",
        "        return self.data[self.column].apply(lambda row: re.sub(\"@[A-Za-z0-9]+_[A-Za-z0-9]+\",\"\",row))\n",
        "\n",
        "    def remove_end_line(self,data,column):\n",
        "\n",
        "        return data[column].str.replace('\\n','')\n",
        "\n",
        "    def concat(self):\n",
        "\n",
        "        self.data[self.label] = self.tag[self.label]\n",
        "\n",
        "        return self.data\n",
        "    \n",
        "    def convert_int(self):\n",
        "\n",
        "      return self.tag[self.label].apply(lambda x: int(x))\n",
        "\n",
        "\n",
        "    def clean_data(self):\n",
        "\n",
        "        text_column = self.column\n",
        "        label_column = self.label\n",
        "\n",
        "        # Rename columns in both label and text data\n",
        "        self.data = self.rename_columns(self.data,text_column)\n",
        "\n",
        "        self.tag = self.rename_columns(self.tag, label_column)\n",
        "\n",
        "        # Remove Punctuation\n",
        "        self.data[text_column] = self.remove_punctuation(self.data,text_column) \n",
        "\n",
        "        # Lower words in text data\n",
        "        self.data[text_column] = self.lower()\n",
        "\n",
        "        # Remove user mentions in text data\n",
        "        self.data[text_column] = self.remove_mentions()\n",
        "\n",
        "        # Remove end line character from label and text data\n",
        "        self.data[text_column] = self.remove_end_line(self.data,text_column)\n",
        "\n",
        "        self.tag[label_column] = self.remove_end_line(self.tag,label_column)\n",
        "\n",
        "        # Convert label to int\n",
        "        self.tag[label_column] = self.convert_int()\n",
        "        \n",
        "\n",
        "        # Concat text and labels\n",
        "\n",
        "        return self.concat()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL13JLtNss_6"
      },
      "source": [
        "<h1>Preprocess Slovenian</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vdIT6Wwsx7I"
      },
      "source": [
        "class Slovenian():\n",
        "    def __init__(self,path):\n",
        "        self.path = path\n",
        "        self.data = self.read_data()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "        return pd.read_csv(self.path)\n",
        "\n",
        "    def rename_columns(self):\n",
        "\n",
        "        self.data = self.data[['text','type']]\n",
        "        return self.data.rename(columns={'type':self.label})\n",
        "\n",
        "    def strip_punctuation(self):\n",
        "        return self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
        "\n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def remove_stopwords(self, data, column):\n",
        "        data[column] = data[column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"slovene\")])\n",
        "        return data[column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def change_labels(self,x):\n",
        "\n",
        "        change = {'Background offensive':'offensive', 'Acceptable speech':'not offensive', 'Background violence':'violence',\n",
        "                  'Other offensive':'offensive', 'Inappropriate':'innappropriate', 'Other violence':'violence'}\n",
        "\n",
        "        for k, v in change.items():\n",
        "\n",
        "             x = x.replace(k, v)\n",
        "        return x\n",
        "\n",
        "    def convert_labels(self):\n",
        "\n",
        "        self.data[self.label] = self.data[self.label].apply(lambda x: self.change_labels(x))\n",
        "\n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 1 if x=='offensive' else 0)\n",
        "\n",
        "\n",
        "    def clean_data(self):\n",
        "\n",
        "        # Rename Columns\n",
        "        self.data = self.rename_columns()\n",
        "\n",
        "        # Strip Punctuation\n",
        "        self.data[self.column] = self.strip_punctuation()\n",
        "\n",
        "        # Lowercase\n",
        "        self.data[self.column] = self.lower()\n",
        "\n",
        "        # Remove stopwords\n",
        "        self.data[self.column] = self.remove_stopwords(self.data,self.column)\n",
        "\n",
        "        self.convert_labels()\n",
        "\n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T3LPhIf1cCI"
      },
      "source": [
        "<h1>Preprocess Croatian</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wt4dzz51gj-"
      },
      "source": [
        "class Croatian():\n",
        "    def __init__(self,path):\n",
        "        self.path = path\n",
        "        self.data = self.read_data()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "        return pd.read_csv(self.path)\n",
        "\n",
        "    def rename_columns(self):\n",
        "\n",
        "        self.data = self.data[['text','type']]\n",
        "        return self.data.rename(columns={'type':self.label})\n",
        "\n",
        "    def strip_punctuation(self):\n",
        "        return self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
        "\n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def remove_stopwords(self, data, column):\n",
        "        data[column] = data[column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"slovene\")])\n",
        "        return data[column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def change_labels(self,x):\n",
        "\n",
        "        change = {'Background offensive':'offensive', 'Acceptable speech':'not offensive', 'Background violence':'offensive',\n",
        "                  'Other offensive':'offensive', 'Inappropriate':'offensive', 'Other violence':'offensive'}\n",
        "\n",
        "        for k, v in change.items():\n",
        "\n",
        "             x = x.replace(k, v)\n",
        "        return x\n",
        "\n",
        "    def convert_labels(self):\n",
        "\n",
        "        self.data[self.label] = self.data[self.label].apply(lambda x: self.change_labels(x))\n",
        "\n",
        "        hate = ['offensive','not offensive']\n",
        "\n",
        "        self.data = self.data[self.data[self.label].isin(hate)]\n",
        "    \n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 1 if x=='offensive' else 0)\n",
        "\n",
        "\n",
        "    def clean_data(self):\n",
        "\n",
        "        # Rename Columns\n",
        "        self.data = self.rename_columns()\n",
        "\n",
        "        # Strip Punctuation\n",
        "        self.data[self.column] = self.strip_punctuation()\n",
        "\n",
        "        # Lowercase\n",
        "        self.data[self.column] = self.lower()\n",
        "\n",
        "        # Remove stopwords\n",
        "        self.data[self.column] = self.remove_stopwords(self.data,self.column)\n",
        "\n",
        "        self.convert_labels()\n",
        "\n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK-kNPlMs9R0"
      },
      "source": [
        "<h1>BERT Tokenizer, Tensors and Dataloaders</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1GJm9B-s9rw"
      },
      "source": [
        "class Transform_Data():\n",
        "  def __init__(self,train,test,Bert_model,column,label):\n",
        "    self.max_length = 60\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(Bert_model)\n",
        "    self.train = train\n",
        "    self.test = test\n",
        "    self.column = column\n",
        "    self.label = label\n",
        "\n",
        "  def BERTTokenizer(self):\n",
        "\n",
        "    train_encodings = self.tokenizer.batch_encode_plus(self.train[self.column].tolist(),add_special_tokens = True, truncation=True, padding=True, max_length=self.max_length,return_tensors='pt')\n",
        "    train_y = torch.tensor(self.train[self.label].tolist())\n",
        "  \n",
        "    val_encodings = self.tokenizer.batch_encode_plus(self.test[self.column].tolist(),add_special_tokens = True, truncation=True, padding=True, max_length=self.max_length,return_tensors='pt')\n",
        "    val_y = torch.tensor(self.test[self.label].tolist())\n",
        "\n",
        "    return train_encodings, train_y, val_encodings,val_y \n",
        "\n",
        "  def Tensors_and_DataLoaders(self,train_encodings,train_y,val_encodings, val_y):\n",
        "\n",
        "    #====================\n",
        "    #Train data\n",
        "    #====================\n",
        "    train_data = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_y)\n",
        "\n",
        "    #train_sampler = RandomSampler(train_data)\n",
        "    #train_sampler = ImbalancedDatasetSampler(train_data)\n",
        "    #train_sampler = WeightedRandomSampler()\n",
        "    class_samples = [(train_y == 0.).sum(dim=0),(train_y == 1.).sum(dim=0)]\n",
        "    total_samples = sum(class_samples)\n",
        "    \n",
        "\n",
        "    class_weights = [total_samples/class_samples[i] for i in range(len(class_samples))]\n",
        "    weights = [class_weights[train_y[i]] for i in range(int(total_samples))]\n",
        "\n",
        "    train_sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(total_samples))\n",
        "    \n",
        "\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "\n",
        "    #====================\n",
        "    #Test/Validation data\n",
        "    #====================\n",
        "\n",
        "    val_data = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_y)\n",
        "\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=32)\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "  def Execute(self):\n",
        "\n",
        "    train_encodings, train_y, val_encodings,val_y = self.BERTTokenizer()\n",
        "    train_dataloader, val_dataloader = self.Tensors_and_DataLoaders(train_encodings,train_y,val_encodings, val_y)\n",
        "    return train_encodings, train_y, val_encodings,val_y, train_dataloader, val_dataloader\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG2v9xb0s_jK"
      },
      "source": [
        "<H1>BERT Model</H1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2F_uHJatBwa"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert,freeze_bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      self.freeze_bert = freeze_bert\n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.3)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,1)\n",
        "      \n",
        "\n",
        "      # sigmoid activation function\n",
        "      self.sigmoid =  nn.Sigmoid()\n",
        "\n",
        "      if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask,return_dict=False) #,return_dict=False\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "      \n",
        "      # activation function\n",
        "      #x = self.relu(x)\n",
        "      \n",
        "      # dropout\n",
        "      #x = self.dropout(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.sigmoid(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjNlb2rJtDgw"
      },
      "source": [
        "<h1>Optimizer and Scheduler</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiBi637atFFU"
      },
      "source": [
        "from transformers import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "\n",
        "def Optimizer_Scheduler(model,train_dataloader,num_epochs):\n",
        "  \n",
        "  optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        " \n",
        "  num_training_steps = num_epochs * len(train_dataloader)\n",
        "  lr_scheduler = get_scheduler(\n",
        "                       \"linear\",\n",
        "                       optimizer=optimizer,\n",
        "                       num_warmup_steps=0,\n",
        "                       num_training_steps=num_training_steps)\n",
        "\n",
        "  return optimizer, lr_scheduler, num_training_steps"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZneFnDJtJIH"
      },
      "source": [
        "<h1>Train Model</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAQh4UI1tJaX"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def Mean(data):\n",
        "    return sum(data) / len(data)\n",
        "\n",
        "def train(model, num_epochs, train_dataloader,val_dataloader,best_valid_loss,language,new):\n",
        "  \n",
        "  progress_bar = tqdm(range(num_training_steps))\n",
        "  loss_fn = nn.BCELoss()\n",
        "  #loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  # Initialize arrays\n",
        "  train_acc = []\n",
        "  val_acc = [] \n",
        "  train_loss = []  \n",
        "  val_loss = []\n",
        "  avg_acc_0 = []\n",
        "  avg_acc_1 = []\n",
        "  total_avg_loss_train = []\n",
        "  total_avg_loss_val = []\n",
        "\n",
        "  # Set a flag for results\n",
        "  flag = True\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "    \n",
        "    total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "    predictions = []\n",
        "    real_label = []\n",
        "\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      batch_counts += 1\n",
        "      \n",
        "      # batch to GPU\n",
        "      batch = [r.to(device) for r in batch]\n",
        "\n",
        "      sent_id, mask, labels = batch\n",
        "      real_label.append(labels.detach().cpu().numpy())\n",
        "      \n",
        "      \n",
        "      # clear previous gradients\n",
        "      model.zero_grad()\n",
        "       \n",
        "      # predictions for current batch\n",
        "      output = model(sent_id, mask)\n",
        "\n",
        "      pred = torch.round(output)\n",
        "\n",
        "      pred = pred.detach().cpu().numpy()\n",
        "      pred = pred.flatten()\n",
        "      predictions.append(pred)\n",
        "      \n",
        "      \n",
        "      # compute loss for current batch\n",
        "      loss = loss_fn(output, labels.unsqueeze(1).float())\n",
        "\n",
        "      # add total loss\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "\n",
        "      # backpropagation to calculate gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # prevent the exploding gradient problem\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "\n",
        "      # update parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      # update scheduler \n",
        "      lr_scheduler.step()\n",
        "\n",
        "      # clear optimizer gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      progress_bar.update(1)\n",
        "\n",
        "    # compute training loss of each batch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # flatten labels and predictions\n",
        "    flat_label = np.concatenate(real_label).astype(int).ravel().tolist()\n",
        "    \n",
        "    flat_predictions = np.concatenate(predictions).astype(int).ravel().tolist()\n",
        "\n",
        "    # Append accuracy and validation loss for training data\n",
        "    train_acc.append(f1_score(flat_label,flat_predictions))\n",
        "    train_loss.append(avg_loss)\n",
        "\n",
        "    # Validation\n",
        "    val_flat_label,val_flat_predictions, val_avg_loss = validate(model,val_dataloader,loss_fn)\n",
        "\n",
        "    if val_avg_loss < best_valid_loss:\n",
        "        best_valid_loss = val_avg_loss\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/Datasets/All/best_same_'+language+'_weights.pt')\n",
        "        new.append(model.state_dict())\n",
        "        \n",
        "    \n",
        "    \n",
        "    # Append accuracy and validation loss for validation data\n",
        "    val_acc.append(f1_score(val_flat_label,val_flat_predictions))\n",
        "    val_loss.append(val_avg_loss)\n",
        "\n",
        "    # Compute each class Accuracy (validation set)\n",
        "    acc_0,acc_1 = Class_F1_score(val_flat_label,val_flat_predictions)\n",
        "    avg_acc_0.append(acc_0)\n",
        "    avg_acc_1.append(acc_1)\n",
        "\n",
        "    # Print table with insights\n",
        "    if flag:\n",
        "      print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Train F1-score':^9} | {'Val F1-score':^9} | {'Class_0 F1-score':^10} | {'Class_1 F1-score':^10} \")\n",
        "      flag = False\n",
        "\n",
        "    print(\"-\"*85)\n",
        "    print(f\"{str(epoch + 1) +'/'+ str(num_epochs):^7} | {avg_loss:^12.6f} | {val_avg_loss:^10.6f} | {f1_score(flat_label,flat_predictions):^9.2f} | {f1_score(val_flat_label,val_flat_predictions):^9.2f} | {acc_0:^10.2f} | {acc_1:^10.2f} \")\n",
        "  \n",
        "  \n",
        "\n",
        "  # Print mean of Accuracy and Loss\n",
        "  print(\"-\"*85)\n",
        "  print(f\"{'Average':^7} | {Mean(train_loss):^12.6f} | {Mean(val_loss):^10.6f} | {Mean(train_acc):^9.2f} | {Mean(val_acc):^9.2f} | {Mean(avg_acc_0):^10.2f} | {Mean(avg_acc_1):^10.2f} \")\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  return train_loss,val_loss, Mean(train_loss), Mean(val_loss), best_valid_loss, new\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZPNZGBntLPd"
      },
      "source": [
        "<h1>Evaluate Model</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E33MBqxytPvE"
      },
      "source": [
        "def validate(model,val_dataloader,loss_fn):\n",
        "  \n",
        "    \n",
        "\n",
        "    model.eval() \n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    \n",
        "    val_preds = []\n",
        "    val_label = []\n",
        "    \n",
        "    for step,batch in enumerate(val_dataloader):\n",
        "      \n",
        "      batch = [t.to(device) for t in batch]\n",
        "      \n",
        "      sent_id, mask, labels = batch\n",
        "      val_label.append(labels.detach().cpu().numpy())\n",
        "      \n",
        "      with torch.no_grad():\n",
        "        output = model(sent_id,mask)\n",
        "        \n",
        "        loss = loss_fn(output,labels.unsqueeze(1).float())     \n",
        "\n",
        "        total_loss = total_loss + loss.item()\n",
        "        \n",
        "      pred = torch.round(output)\n",
        "      pred = pred.detach().cpu().numpy()\n",
        "      pred = pred.flatten()\n",
        "      val_preds.append(pred)\n",
        "\n",
        "    # compute training loss of each epoch\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "\n",
        "    # flatten labels and predictions\n",
        "    flat_label = np.concatenate(val_label).astype(int).ravel().tolist()\n",
        "    \n",
        "    \n",
        "    flat_predictions = np.concatenate(val_preds).astype(int).ravel().tolist()\n",
        "\n",
        "    \n",
        "\n",
        "    return flat_label,flat_predictions, avg_loss"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W85QC4auozou"
      },
      "source": [
        "<h1>Class F1-score</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6jxjy4Xo2HK"
      },
      "source": [
        "def Class_F1_score(val_y,new_preds):\n",
        "\n",
        "  report = classification_report(val_y,new_preds, output_dict=True )\n",
        "\n",
        "  return report['0']['f1-score'], report['1']['f1-score']"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvYVdNYRtTFL"
      },
      "source": [
        "<h1>Class Accuracy</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX8uz0e3tTeu"
      },
      "source": [
        "import numpy\n",
        "\n",
        "def Class_Accuracy_1(all):\n",
        "  # Class 1 Accuracy\n",
        "\n",
        "  all['new'] = all.apply(lambda x: True if x['predicted label']==1 and x['real label']==1 else False, axis=1)\n",
        "\n",
        "  return len(all[all['new']==True])/len(all[all['real label']==1])\n",
        "\n",
        "def Class_Accuracy_0(all):\n",
        "  # Class 0 Accuracy\n",
        "\n",
        "  all['new'] = all.apply(lambda x: True if x['predicted label']==0 and x['real label']==0 else False, axis=1)\n",
        "\n",
        "  return len(all[all['new']==True])/len(all[all['real label']==0])\n",
        "\n",
        "\n",
        "def Class_Accuracy(val_y,new_preds):\n",
        "  real = pd.DataFrame(numpy.asarray(val_y))\n",
        "  real = real.rename(columns={0:'real label'})\n",
        "\n",
        "  preds = pd.DataFrame(new_preds)\n",
        "  preds = preds.rename(columns={0:'predicted label'})\n",
        "  \n",
        "  all = pd.concat([real,preds],axis=1)\n",
        "\n",
        "  acc_1 = Class_Accuracy_1(all)\n",
        "  acc_0 = Class_Accuracy_0(all)\n",
        "\n",
        "  return acc_0,acc_1\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt7YylVzterO"
      },
      "source": [
        "<h1>Choose Language</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RikX3NGtgee"
      },
      "source": [
        "def language_model(lang):\n",
        "\n",
        "  if lang=='multilingual':\n",
        "    return 'bert-base-multilingual-cased'\n",
        "  \n",
        "  else:\n",
        "    return \"dkleczek/bert-base-polish-uncased-v1\""
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e094o4hIZEIt"
      },
      "source": [
        "<h1>Undersample</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt0yswnAZHes"
      },
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "import numpy as np\n",
        "import random\n",
        "from past.builtins import xrange\n",
        "\n",
        "def kMedoids(D, k, tmax=100):\n",
        "    # determine dimensions of distance matrix D\n",
        "    m, n = D.shape\n",
        "\n",
        "    if k > n:\n",
        "        raise Exception('too many medoids')\n",
        "\n",
        "    # find a set of valid initial cluster medoid indices since we\n",
        "    # can't seed different clusters with two points at the same location\n",
        "    valid_medoid_inds = set(range(n))\n",
        "    invalid_medoid_inds = set([])\n",
        "    rs,cs = np.where(D==0)\n",
        "    # the rows, cols must be shuffled because we will keep the first duplicate below\n",
        "    index_shuf = list(range(len(rs)))\n",
        "    np.random.shuffle(index_shuf)\n",
        "    rs = rs[index_shuf]\n",
        "    cs = cs[index_shuf]\n",
        "    for r,c in zip(rs,cs):\n",
        "        # if there are two points with a distance of 0...\n",
        "        # keep the first one for cluster init\n",
        "        if r < c and r not in invalid_medoid_inds:\n",
        "            invalid_medoid_inds.add(c)\n",
        "    valid_medoid_inds = list(valid_medoid_inds - invalid_medoid_inds)\n",
        "\n",
        "    if k > len(valid_medoid_inds):\n",
        "        raise Exception('too many medoids (after removing {} duplicate points)'.format(\n",
        "            len(invalid_medoid_inds)))\n",
        "\n",
        "    # randomly initialize an array of k medoid indices\n",
        "    M = np.array(valid_medoid_inds)\n",
        "    np.random.shuffle(M)\n",
        "    M = np.sort(M[:k])\n",
        "\n",
        "    # create a copy of the array of medoid indices\n",
        "    Mnew = np.copy(M)\n",
        "\n",
        "    # initialize a dictionary to represent clusters\n",
        "    C = {}\n",
        "    for t in xrange(tmax):\n",
        "        # determine clusters, i. e. arrays of data indices\n",
        "        J = np.argmin(D[:,M], axis=1)\n",
        "        for kappa in range(k):\n",
        "            C[kappa] = np.where(J==kappa)[0]\n",
        "        # update cluster medoids\n",
        "        for kappa in range(k):\n",
        "            J = np.mean(D[np.ix_(C[kappa],C[kappa])],axis=1)\n",
        "            j = np.argmin(J)\n",
        "            Mnew[kappa] = C[kappa][j]\n",
        "        np.sort(Mnew)\n",
        "        # check for convergence\n",
        "        if np.array_equal(M, Mnew):\n",
        "            break\n",
        "        M = np.copy(Mnew)\n",
        "    else:\n",
        "        # final update of cluster memberships\n",
        "        J = np.argmin(D[:,M], axis=1)\n",
        "        for kappa in range(k):\n",
        "            C[kappa] = np.where(J==kappa)[0]\n",
        "\n",
        "    # return results\n",
        "    return M, C\n",
        "\n",
        "def compare(ratio,data,balance):\n",
        "    class_tags = data['label'].value_counts().to_list()\n",
        "    if ratio>=0.5:\n",
        "        total = balance - min(class_tags)\n",
        "        balance_class_index = class_tags.index(max(class_tags))\n",
        "        class_index = class_tags.index(min(class_tags))\n",
        "\n",
        "        new = data[data['label']==balance_class_index]\n",
        "        print(new['label'].value_counts())\n",
        "\n",
        "        model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "        embeddings = model.encode(new['text'].to_numpy())\n",
        "\n",
        "        D = pairwise_distances(embeddings, metric='cosine')\n",
        "\n",
        "        M, C = kMedoids(D, total)\n",
        "        data1 = new.iloc[M]\n",
        "    \n",
        "        data2 = data[data['label']==class_index]\n",
        "        return pd.concat([data1,data2],ignore_index=True)\n",
        "    else:\n",
        "      model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "      embeddings = model.encode(data['text'].to_numpy())\n",
        "\n",
        "      D = pairwise_distances(embeddings, metric='cosine')\n",
        "\n",
        "      M, C = kMedoids(D, balance)\n",
        "      return data.iloc[M]\n",
        "\n",
        "\n",
        "def check_imbalance_ratio(data):\n",
        "    class_tags = data['label'].value_counts().to_list()\n",
        "    difference = max(class_tags) - min(class_tags)\n",
        "    ratio = difference/max(class_tags)\n",
        "    return ratio\n",
        "\n",
        "def balance(data,sample):\n",
        "    return compare(check_imbalance_ratio(data),data,sample)\n",
        "\n",
        "def execute(data,sample):\n",
        "   \n",
        "    return balance(data,sample)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLR4knQxtjiB"
      },
      "source": [
        "<h1>Train</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfC8oQyCtlZq",
        "outputId": "ad2d07dc-0159-4713-a3cd-1a073b5c0d21"
      },
      "source": [
        "text_path = \"/content/drive/My Drive/Datasets/Slovenian/Slovene_train_set.csv\"\n",
        "\n",
        "slovenian = Slovenian(text_path)\n",
        "slovenian_data = slovenian.clean_data()\n",
        "\n",
        "text_path = \"/content/drive/My Drive/Datasets/Polish/training_set_clean_only_text.txt\"\n",
        "tag_path = \"/content/drive/My Drive/Datasets/Polish/training_set_clean_only_tags.txt\"\n",
        "\n",
        "polish = Polish(text_path,tag_path)\n",
        "polish_data = polish.clean_data()\n",
        "print(len(polish_data))\n",
        "\n",
        "polish_data = execute(polish_data,7990)\n",
        "\n",
        "\n",
        "text_path = \"/content/drive/My Drive/Datasets/Croatian/Croatian_train_set.csv\"\n",
        "croatian = Croatian(text_path)\n",
        "croatian_data = croatian.clean_data()\n",
        "print(len(croatian_data))\n",
        "croatian_data  = execute(croatian_data,7990)\n",
        "\n",
        "text_path = \"/content/drive/My Drive/Datasets/English/English_8743_train.csv\"\n",
        "\n",
        "#english = English(text_path)\n",
        "#english_data = english.clean_data()\n",
        "#english_data  = execute(english_data,7990)\n",
        "\n",
        "data = pd.concat([polish_data,croatian_data,slovenian_data])\n",
        "\n",
        "\n",
        "kfold=KFold(n_splits=5,shuffle=True,random_state=23)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "accuracy = []\n",
        "fold_loss_train = []\n",
        "fold_loss_val = []\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "language = 'multilingual'\n",
        "Bert_model = language_model(language)\n",
        "\n",
        "new= []"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10041\n",
            "0    9190\n",
            "Name: label, dtype: int64\n",
            "8851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347,
          "referenced_widgets": [
            "244ec308dd0846ef8105a9edf8e0a785",
            "3e67725de6054b3db97a2cdcc6919173",
            "5fc7022818334f38a46512a44324f1ed",
            "01662fd16757448d9276b16c78810adf",
            "15448eb44eda4569a0d61b654be68c64",
            "1aceb5be6f1540f48a1aedbf250e247d",
            "5b6b4ba8f6594e1f88ec62b5791b555b",
            "4d63f74ef0c943588ecdfeb5c2e2da13",
            "dd056c4e5c724145ab6cc919b8c00ec6",
            "4a68711b83e54ccabd185305c3d6611b",
            "f3f7a797682a46a5ac8e553857f2c0e9"
          ]
        },
        "id": "chC16FBTuyLl",
        "outputId": "64a3db80-6322-4f07-ebb2-7f9efe62abd8"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_dataset,val_dataset = train_test_split(data,test_size=0.15,random_state=21)\n",
        "  \n",
        "  \n",
        "# Transfom Data\n",
        "transform = Transform_Data(train_dataset,val_dataset,Bert_model,'text','label')\n",
        "train_encodings, train_y, val_encodings,val_y, train_dataloader, val_dataloader = transform.Execute()\n",
        "\n",
        "# Bert Model\n",
        "bert = BertModel.from_pretrained(Bert_model)\n",
        "model = BERT_Arch(bert,False)\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer, lr_scheduler, num_training_steps = Optimizer_Scheduler(model,train_dataloader,num_epochs)\n",
        "\n",
        "# Train Model\n",
        "train_loss,val_loss, total_loss_train, total_loss_val, best_valid_loss, new = train(model, num_epochs, train_dataloader,val_dataloader,best_valid_loss,language,new)\n",
        "fold_loss_train.append(total_loss_train)\n",
        "fold_loss_val.append(total_loss_val)\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "244ec308dd0846ef8105a9edf8e0a785",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3185 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch  |  Train Loss  |  Val Loss  | Train F1-score | Val F1-score | Class_0 F1-score | Class_1 F1-score \n",
            "-------------------------------------------------------------------------------------\n",
            "  1/5   |   0.493329   |  0.674940  |   0.78    |   0.71    |    0.68    |    0.71    \n",
            "-------------------------------------------------------------------------------------\n",
            "  2/5   |   0.372824   |  0.596239  |   0.84    |   0.74    |    0.78    |    0.74    \n",
            "-------------------------------------------------------------------------------------\n",
            "  3/5   |   0.266230   |  0.603857  |   0.90    |   0.69    |    0.80    |    0.69    \n",
            "-------------------------------------------------------------------------------------\n",
            "  4/5   |   0.193209   |  0.707355  |   0.94    |   0.71    |    0.80    |    0.71    \n",
            "-------------------------------------------------------------------------------------\n",
            "  5/5   |   0.147557   |  0.829226  |   0.95    |   0.70    |    0.80    |    0.70    \n",
            "-------------------------------------------------------------------------------------\n",
            "Average |   0.294630   |  0.682323  |   0.88    |   0.71    |    0.77    |    0.71    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjwoTXSN-xIl"
      },
      "source": [
        "def predict(model,test_seq,test_mask,test_y):\n",
        "  torch.cuda.empty_cache()\n",
        "  \n",
        "  \n",
        "  path = '/content/drive/My Drive/Datasets/Slovene_Polish_Croatian/best_Loss_multilingual_weights.pt'\n",
        "  model.load_state_dict(torch.load(path))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    preds = model(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "  \n",
        "  preds = numpy.round(preds)\n",
        "  final = np.concatenate(preds).astype(int).ravel().tolist()\n",
        "  #print(final)\n",
        "\n",
        "  return final"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cNCoHYVJcBD"
      },
      "source": [
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    path = '/content/drive/My Drive/Datasets/All/best_same_multilingual_weights.pt'\n",
        "    model.load_state_dict(torch.load(path))\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "            #preds = logits.detach().cpu().numpy()\n",
        "            \n",
        "        all_logits.append(torch.round(logits))\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "    preds = all_logits.detach().cpu().numpy()\n",
        "    preds = np.concatenate(preds).astype(int).ravel().tolist()\n",
        "    \n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    #probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return preds"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X23ylUfu--YX",
        "outputId": "d7946cd0-3185-4567-a5e9-4fe1891b545a"
      },
      "source": [
        "import numpy\n",
        "import torch, gc\n",
        "\n",
        "# Test paths\n",
        "#text_path = \"/content/drive/My Drive/Datasets/Slovenian/Slovene_test_set.csv\"\n",
        "\n",
        "\n",
        "# Clean data\n",
        "#slovenian = Slovenian(text_path)\n",
        "#test_data = slovenian.clean_data()\n",
        "\n",
        "# paths\n",
        "text_path = \"/content/drive/My Drive/Datasets/Polish/test_set_clean_only_text.txt\"\n",
        "tag_path = \"/content/drive/My Drive/Datasets/Polish/test_set_clean_only_tags.txt\"\n",
        "\n",
        "# Clean data\n",
        "#polish = Polish(text_path,tag_path)\n",
        "#test_data = polish.clean_data()\n",
        "\n",
        "#Clean data\n",
        "text_path = \"/content/drive/My Drive/Datasets/Croatian/Croatian_test_set.csv\"\n",
        "croatian = Croatian(text_path)\n",
        "test_data = croatian.clean_data()\n",
        "\n",
        "\n",
        "#test_data = pd.concat([polish_data,slovenian_data,croatian_data])\n",
        "\n",
        "language = 'multilingual'\n",
        "Bert_model = language_model(language)\n",
        "\n",
        "# Tokenize Data\n",
        "tokenizer = BertTokenizer.from_pretrained(Bert_model)\n",
        "\n",
        "test_encodings = tokenizer.batch_encode_plus(test_data['text'].tolist(),add_special_tokens = True, truncation=True, padding=True, max_length=60,return_tensors='pt')\n",
        "\n",
        "test_seq = torch.tensor(test_encodings['input_ids'])\n",
        "test_mask = torch.tensor(test_encodings['attention_mask'])\n",
        "\n",
        "test_dataset = TensorDataset(test_seq, test_mask)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n",
        "\n",
        "test_y = torch.tensor(test_data['label'].tolist())\n",
        "\n",
        "bert = BertModel.from_pretrained(Bert_model)\n",
        "model = BERT_Arch(bert,False)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "test_pred = bert_predict(model, test_dataloader)\n",
        "#test_pred = predict(model,test_seq,test_mask,test_y)\n",
        "test_label = test_y.detach().cpu().numpy()\n",
        "print(len(test_label))\n",
        "print(len(test_pred))\n",
        "print(classification_report(test_label,test_pred))\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2120\n",
            "2120\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.54      0.63       829\n",
            "           1       0.75      0.89      0.81      1291\n",
            "\n",
            "    accuracy                           0.75      2120\n",
            "   macro avg       0.75      0.71      0.72      2120\n",
            "weighted avg       0.75      0.75      0.74      2120\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6q9vD-XLSwv",
        "outputId": "923c0a32-2be1-4461-e569-1a18fbde17b3"
      },
      "source": [
        "print(len(test_label))\n",
        "print(test_pred)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2120\n",
            "[1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n"
          ]
        }
      ]
    }
  ]
}