{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLM_ROBERTA Polish,Slovenian,Croatian.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMHE9+or4hxUiulNdg/RfDX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlavrent/Multilingual-Hate-Speech-Detection/blob/main/XLM_ROBERTA_Polish%2CSlovenian%2CCroatian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbnzyy25P6bA"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3Jvc37Trg3B"
      },
      "source": [
        "pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW98BXjgQhrr"
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiYEUcDtQil-"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOA5DdeYQng2"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "  print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7aJETliQprv"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#from torchsampler import ImbalancedDatasetSampler\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "from sklearn.model_selection import KFold\n",
        "import torch, gc\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from sklearn.metrics import f1_score,classification_report\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22zIRNZwnr_O"
      },
      "source": [
        "<h1>Preprocess Polish</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOviFAICQsD0"
      },
      "source": [
        "class Polish():\n",
        "\n",
        "    def __init__(self,data_path,tag_path):\n",
        "        self.path = data_path\n",
        "        self.tag_path = tag_path\n",
        "        self.data = self.read_data()\n",
        "        self.tag = self.read_tag()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "\n",
        "        open_data = open(self.path, \"r\", encoding=\"utf8\")\n",
        "\n",
        "        return pd.DataFrame(open_data)\n",
        "\n",
        "    def read_tag(self):\n",
        "\n",
        "        open_tag = open(self.tag_path,'r')\n",
        "\n",
        "        return pd.DataFrame(open_tag)\n",
        "\n",
        "    def remove_punctuation(self,data,column):\n",
        "      \n",
        "      return data[column].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\n",
        "  \n",
        "    def lower(self):\n",
        "\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def rename_columns(self,data,column):\n",
        "\n",
        "        return data.rename(columns={0:column})\n",
        "\n",
        "    def remove_mentions(self):\n",
        "\n",
        "        return self.data[self.column].apply(lambda row: re.sub(\"@[A-Za-z0-9]+_[A-Za-z0-9]+\",\"\",row))\n",
        "\n",
        "    def remove_end_line(self,data,column):\n",
        "\n",
        "        return data[column].str.replace('\\n','')\n",
        "\n",
        "    def concat(self):\n",
        "\n",
        "        self.data[self.label] = self.tag[self.label]\n",
        "\n",
        "        return self.data\n",
        "    \n",
        "    def convert_int(self):\n",
        "\n",
        "      return self.tag[self.label].apply(lambda x: int(x))\n",
        "\n",
        "\n",
        "    def clean_data(self):\n",
        "\n",
        "        text_column = self.column\n",
        "        label_column = self.label\n",
        "\n",
        "        # Rename columns in both label and text data\n",
        "        self.data = self.rename_columns(self.data,text_column)\n",
        "\n",
        "        self.tag = self.rename_columns(self.tag, label_column)\n",
        "\n",
        "        # Remove Punctuation\n",
        "        self.data[text_column] = self.remove_punctuation(self.data,text_column) \n",
        "\n",
        "        # Lower words in text data\n",
        "        self.data[text_column] = self.lower()\n",
        "\n",
        "        # Remove user mentions in text data\n",
        "        self.data[text_column] = self.remove_mentions()\n",
        "\n",
        "        # Remove end line character from label and text data\n",
        "        self.data[text_column] = self.remove_end_line(self.data,text_column)\n",
        "\n",
        "        self.tag[label_column] = self.remove_end_line(self.tag,label_column)\n",
        "\n",
        "        # Convert label to int\n",
        "        self.tag[label_column] = self.convert_int()\n",
        "        \n",
        "\n",
        "        # Concat text and labels\n",
        "\n",
        "        return self.concat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMjctOUWn7RQ"
      },
      "source": [
        "<h1>Preprocess Slovenian</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3Z5Cj8SQt7p"
      },
      "source": [
        "class Slovenian():\n",
        "    def __init__(self,path):\n",
        "        self.path = path\n",
        "        self.data = self.read_data()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "        return pd.read_csv(self.path)\n",
        "\n",
        "    def rename_columns(self):\n",
        "\n",
        "        self.data = self.data[['text','type']]\n",
        "        return self.data.rename(columns={'type':self.label})\n",
        "\n",
        "    def strip_punctuation(self):\n",
        "        return self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
        "\n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def remove_stopwords(self, data, column):\n",
        "        data[column] = data[column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"slovene\")])\n",
        "        return data[column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def change_labels(self,x):\n",
        "\n",
        "        change = {'Background offensive':'offensive', 'Acceptable speech':'not offensive', 'Background violence':'violence',\n",
        "                  'Other offensive':'offensive', 'Inappropriate':'innappropriate', 'Other violence':'violence'}\n",
        "\n",
        "        for k, v in change.items():\n",
        "\n",
        "             x = x.replace(k, v)\n",
        "        return x\n",
        "\n",
        "    def convert_labels(self):\n",
        "\n",
        "        self.data[self.label] = self.data[self.label].apply(lambda x: self.change_labels(x))\n",
        "\n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 1 if x=='offensive' else 0)\n",
        "\n",
        "\n",
        "    def clean_data(self):\n",
        "\n",
        "        # Rename Columns\n",
        "        self.data = self.rename_columns()\n",
        "\n",
        "        # Strip Punctuation\n",
        "        self.data[self.column] = self.strip_punctuation()\n",
        "\n",
        "        # Lowercase\n",
        "        self.data[self.column] = self.lower()\n",
        "\n",
        "        # Remove stopwords\n",
        "        self.data[self.column] = self.remove_stopwords(self.data,self.column)\n",
        "        \n",
        "        # Convert labels\n",
        "        self.convert_labels()\n",
        "        \n",
        "        # Binarize labels\n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvPd-R_5n_Pg"
      },
      "source": [
        "<h1>Preprocess Croatian</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxh_Z4cHQwxP"
      },
      "source": [
        "class Croatian():\n",
        "    def __init__(self,path):\n",
        "        self.path = path\n",
        "        self.data = self.read_data()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "        return pd.read_csv(self.path)\n",
        "\n",
        "    def rename_columns(self):\n",
        "\n",
        "        self.data = self.data[['text','type']]\n",
        "        return self.data.rename(columns={'type':self.label})\n",
        "\n",
        "    def strip_punctuation(self):\n",
        "        return self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n",
        "\n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def remove_stopwords(self, data, column):\n",
        "        data[column] = data[column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"slovene\")])\n",
        "        return data[column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def change_labels(self,x):\n",
        "\n",
        "        change = {'Background offensive':'offensive', 'Acceptable speech':'not offensive', 'Background violence':'offensive',\n",
        "                  'Other offensive':'offensive', 'Inappropriate':'offensive', 'Other violence':'offensive'}\n",
        "\n",
        "        for k, v in change.items():\n",
        "\n",
        "             x = x.replace(k, v)\n",
        "        return x\n",
        "\n",
        "    def convert_labels(self):\n",
        "\n",
        "        self.data[self.label] = self.data[self.label].apply(lambda x: self.change_labels(x))\n",
        "\n",
        "        hate = ['offensive','not offensive']\n",
        "\n",
        "        self.data = self.data[self.data[self.label].isin(hate)]\n",
        "    \n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 1 if x=='offensive' else 0)\n",
        "\n",
        "\n",
        "    def clean_data(self):\n",
        "\n",
        "        # Rename Columns\n",
        "        self.data = self.rename_columns()\n",
        "\n",
        "        # Strip Punctuation\n",
        "        self.data[self.column] = self.strip_punctuation()\n",
        "\n",
        "        # Lowercase\n",
        "        self.data[self.column] = self.lower()\n",
        "\n",
        "        # Remove stopwords\n",
        "        self.data[self.column] = self.remove_stopwords(self.data,self.column)\n",
        "\n",
        "        self.convert_labels()\n",
        "\n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0-dD8ZBoFHh"
      },
      "source": [
        "<h1>Preprocess Greek</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGmn_vzx3a4X"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "class Greek():\n",
        "    def __init__(self,path):\n",
        "        self.path  = path\n",
        "        self.data = self.read_data()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "        return pd.read_csv(self.path)\n",
        "\n",
        "    def rename_columns(self):\n",
        "\n",
        "        return self.data.rename(columns={'tweet':self.column,'subtask_a':self.label})\n",
        "\n",
        "    def replace_hashtag(self):\n",
        "\n",
        "        return self.data[self.column].apply(lambda x: re.sub(\"#[\\w]+\",\"hashtag\",x))\n",
        "\n",
        "    def strip_punctuation(self):\n",
        "\n",
        "        return self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\n",
        "\n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def remove_stopwords(self, data, column):\n",
        "        data[column] = data[column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"greek\")])\n",
        "        return data[column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 0 if x=='NOT' else 1)\n",
        "\n",
        "    def clean_data(self):\n",
        "        text_column = self.column\n",
        "        label_column = self.label\n",
        "\n",
        "        # Rename Columns\n",
        "        self.data = self.rename_columns()\n",
        "\n",
        "        # Replace hashtag\n",
        "        #self.data[text_column] = self.replace_hashtag()\n",
        "\n",
        "        # Strip Punctuation\n",
        "        self.data[text_column] = self.strip_punctuation()\n",
        "\n",
        "        # Lower text\n",
        "        self.data[text_column] = self.lower()\n",
        "\n",
        "        # Remove Stopwords\n",
        "        #self.data[text_column] = self.remove_stopwords(self.data,text_column)\n",
        "\n",
        "        # Binarize labels\n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUSbFr7LoYtM"
      },
      "source": [
        "<h1>Preprocess English</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1bH83t53oOY"
      },
      "source": [
        "class English():\n",
        "\n",
        "    def __init__(self,path):\n",
        "        self.data = self.read(path)\n",
        "        self.label = 'label'\n",
        "        self.column = 'text'\n",
        "\n",
        "    def read(self,path):\n",
        "        return pd.read_csv(path)\n",
        "\n",
        "    def replace_label(self,x):\n",
        "        if x=='1' or x=='0' or x=='HOF':\n",
        "            return 'HOF'\n",
        "        else:\n",
        "            return 'NOT'\n",
        "\n",
        "    def fix_label(self):\n",
        "        self.data[self.label] = self.data[self.label].apply(lambda x: self.replace_label(x))\n",
        "\n",
        "\n",
        "    def replace_mentions(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda row: re.sub(\"@[A-Za-z0-9]+_*[A-Za-z0-9]+\", \"mention\", row))\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda row: re.sub(\"mention_\", \"mention\", row))\n",
        "\n",
        "    def remove_punctuation(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "    def replace_hashtag(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(\"#[\\w]+\", \"hashtag\", x))\n",
        "\n",
        "    def remove_stopwords(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"english\")])\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def remove_url(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '',x))\n",
        "    \n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 0 if x=='NOT' else 1)\n",
        "\n",
        "    def clean_data(self):\n",
        "        # Fix labels\n",
        "        self.fix_label()\n",
        "\n",
        "        # Remove urls\n",
        "        self.remove_url()\n",
        "\n",
        "        # Replace mentions\n",
        "        self.replace_mentions()\n",
        "\n",
        "        # Replace hashtags\n",
        "        self.replace_hashtag()\n",
        "\n",
        "        # Remove punctuation\n",
        "        self.remove_punctuation()\n",
        "\n",
        "        # Remove stopwords\n",
        "        self.remove_stopwords()\n",
        "\n",
        "        # Lower text\n",
        "        self.lower()\n",
        "\n",
        "        # Binarize labels\n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbmhAi42oho8"
      },
      "source": [
        "<h1>Tokenizer, Tensors and Dataloaders</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTNquqlzQy41"
      },
      "source": [
        "class Transform_Data():\n",
        "  def __init__(self,train,test,ROBERTA_model,column,label):\n",
        "    self.max_length = 60\n",
        "    self.tokenizer = XLMRobertaTokenizer.from_pretrained(ROBERTA_model)\n",
        "    self.train = train\n",
        "    self.test = test\n",
        "    self.column = column\n",
        "    self.label = label\n",
        "\n",
        "  def BERTTokenizer(self):\n",
        "\n",
        "    train_encodings = self.tokenizer.batch_encode_plus(self.train[self.column].tolist(),add_special_tokens = True, truncation=True, padding=True, max_length=self.max_length,return_tensors='pt')\n",
        "    train_y = torch.tensor(self.train[self.label].tolist())\n",
        "  \n",
        "    val_encodings = self.tokenizer.batch_encode_plus(self.test[self.column].tolist(),add_special_tokens = True, truncation=True, padding=True, max_length=self.max_length,return_tensors='pt')\n",
        "    val_y = torch.tensor(self.test[self.label].tolist())\n",
        "\n",
        "    return train_encodings, train_y, val_encodings,val_y \n",
        "\n",
        "  def Tensors_and_DataLoaders(self,train_encodings,train_y,val_encodings, val_y):\n",
        "\n",
        "    #====================\n",
        "    #Train data\n",
        "    #====================\n",
        "    train_data = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_y)\n",
        "    class_samples = [(train_y == 0.).sum(dim=0),(train_y == 1.).sum(dim=0)]\n",
        "    total_samples = sum(class_samples)\n",
        "    \n",
        "\n",
        "    class_weights = [total_samples/class_samples[i] for i in range(len(class_samples))]\n",
        "    weights = [class_weights[train_y[i]] for i in range(int(total_samples))]\n",
        "\n",
        "    train_sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(total_samples))\n",
        "    \n",
        "\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "\n",
        "    #====================\n",
        "    #Test/Validation data\n",
        "    #====================\n",
        "\n",
        "    val_data = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_y)\n",
        "\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=32)\n",
        "\n",
        "    return train_dataloader, val_dataloader\n",
        "\n",
        "  def Execute(self):\n",
        "\n",
        "    train_encodings, train_y, val_encodings,val_y = self.BERTTokenizer()\n",
        "    train_dataloader, val_dataloader = self.Tensors_and_DataLoaders(train_encodings,train_y,val_encodings, val_y)\n",
        "    return train_encodings, train_y, val_encodings,val_y, train_dataloader, val_dataloader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xN1Gnvqolgc"
      },
      "source": [
        "<h1>Output Layer</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGFcA3kAQ58x"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ROBERTA_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, ROBERTA,freeze_bert):\n",
        "      \n",
        "      super(ROBERTA_Arch, self).__init__()\n",
        "\n",
        "      self.bert = ROBERTA\n",
        "      self.freeze_bert = freeze_bert\n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.3)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(self.bert.config.hidden_size,1)\n",
        "      \n",
        "\n",
        "      # sigmoid activation function\n",
        "      self.sigmoid =  nn.Sigmoid()\n",
        "\n",
        "      if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _,cls_hs = self.bert(sent_id, attention_mask=mask,return_dict=False) #,return_dict=False\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "      \n",
        "      # activation function\n",
        "      #x = self.relu(x)\n",
        "      \n",
        "      # dropout\n",
        "      #x = self.dropout(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.sigmoid(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffWF0qTKo3EN"
      },
      "source": [
        "<h1>Optimizer, Scheduler</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NgaShAKRFA1"
      },
      "source": [
        "from transformers import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "\n",
        "def Optimizer_Scheduler(model,train_dataloader,num_epochs):\n",
        "  \n",
        "  optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        " \n",
        "  num_training_steps = num_epochs * len(train_dataloader)\n",
        "  lr_scheduler = get_scheduler(\n",
        "                       \"linear\",\n",
        "                       optimizer=optimizer,\n",
        "                       num_warmup_steps=0,\n",
        "                       num_training_steps=num_training_steps)\n",
        "\n",
        "  return optimizer, lr_scheduler, num_training_steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr7ha0keo9v_"
      },
      "source": [
        "<h1>Train Model</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOj2q8iNRG7h"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def Mean(data):\n",
        "    return sum(data) / len(data)\n",
        "\n",
        "def train(model, num_epochs, train_dataloader,val_dataloader,best_valid_loss,language):\n",
        "  \n",
        "  progress_bar = tqdm(range(num_training_steps))\n",
        "  loss_fn = nn.BCELoss()\n",
        "\n",
        "  # Initialize arrays\n",
        "  train_acc = []\n",
        "  val_acc = [] \n",
        "  train_loss = []  \n",
        "  val_loss = []\n",
        "  avg_acc_0 = []\n",
        "  avg_acc_1 = []\n",
        "  total_avg_loss_train = []\n",
        "  total_avg_loss_val = []\n",
        "\n",
        "  # Set a flag for results\n",
        "  flag = True\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "    \n",
        "    total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "    predictions = []\n",
        "    real_label = []\n",
        "\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      batch_counts += 1\n",
        "      \n",
        "      # batch to GPU\n",
        "      batch = [r.to(device) for r in batch]\n",
        "\n",
        "      sent_id, mask, labels = batch\n",
        "      real_label.append(labels.detach().cpu().numpy())\n",
        "      \n",
        "      \n",
        "      # clear previous gradients\n",
        "      model.zero_grad()\n",
        "       \n",
        "      # predictions for current batch\n",
        "      output = model(sent_id, mask)\n",
        "\n",
        "      pred = torch.round(output)\n",
        "\n",
        "      pred = pred.detach().cpu().numpy()\n",
        "      pred = pred.flatten()\n",
        "      predictions.append(pred)\n",
        "      \n",
        "      \n",
        "      # compute loss for current batch\n",
        "      loss = loss_fn(output, labels.unsqueeze(1).float())\n",
        "\n",
        "      # add total loss\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "\n",
        "      # backpropagation to calculate gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # prevent the exploding gradient problem\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "\n",
        "      # update parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      # update scheduler \n",
        "      lr_scheduler.step()\n",
        "\n",
        "      # clear optimizer gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      progress_bar.update(1)\n",
        "\n",
        "    # compute training loss of each batch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # flatten labels and predictions\n",
        "    flat_label = np.concatenate(real_label).astype(int).ravel().tolist()\n",
        "    \n",
        "    flat_predictions = np.concatenate(predictions).astype(int).ravel().tolist()\n",
        "\n",
        "    # Append accuracy and validation loss for training data\n",
        "    train_acc.append(f1_score(flat_label,flat_predictions))\n",
        "    train_loss.append(avg_loss)\n",
        "\n",
        "    # Validation\n",
        "    val_flat_label,val_flat_predictions, val_avg_loss = validate(model,val_dataloader,loss_fn)\n",
        "\n",
        "    if val_avg_loss < best_valid_loss:\n",
        "        best_valid_loss = val_avg_loss\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/Datasets/All/best_'+language+'_weights.pt')     \n",
        "    \n",
        "    \n",
        "    # Append accuracy and validation loss for validation data\n",
        "    val_acc.append(f1_score(val_flat_label,val_flat_predictions))\n",
        "    val_loss.append(val_avg_loss)\n",
        "\n",
        "    # Compute each class Accuracy (validation set)\n",
        "    acc_0,acc_1 = Class_F1_score(val_flat_label,val_flat_predictions)\n",
        "    avg_acc_0.append(acc_0)\n",
        "    avg_acc_1.append(acc_1)\n",
        "\n",
        "    # Print table with insights\n",
        "    if flag:\n",
        "      print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Train F1-score':^9} | {'Val F1-score':^9} | {'Class_0 F1-score':^10} | {'Class_1 F1-score':^10} \")\n",
        "      flag = False\n",
        "\n",
        "    print(\"-\"*85)\n",
        "    print(f\"{str(epoch + 1) +'/'+ str(num_epochs):^7} | {avg_loss:^12.6f} | {val_avg_loss:^10.6f} | {f1_score(flat_label,flat_predictions):^9.2f} | {f1_score(val_flat_label,val_flat_predictions):^9.2f} | {acc_0:^10.2f} | {acc_1:^10.2f} \")\n",
        "  \n",
        "  \n",
        "\n",
        "  # Print mean of Accuracy and Loss\n",
        "  print(\"-\"*85)\n",
        "  print(f\"{'Average':^7} | {Mean(train_loss):^12.6f} | {Mean(val_loss):^10.6f} | {Mean(train_acc):^9.2f} | {Mean(val_acc):^9.2f} | {Mean(avg_acc_0):^10.2f} | {Mean(avg_acc_1):^10.2f} \")\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  return train_loss,val_loss, Mean(train_loss), Mean(val_loss), best_valid_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhFSbjS1pCaP"
      },
      "source": [
        "<h1>Validate Model</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpM2TODNRJuH"
      },
      "source": [
        "def validate(model,val_dataloader,loss_fn):\n",
        "  \n",
        "    \n",
        "\n",
        "    model.eval() \n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    \n",
        "    val_preds = []\n",
        "    val_label = []\n",
        "    \n",
        "    for step,batch in enumerate(val_dataloader):\n",
        "      \n",
        "      batch = [t.to(device) for t in batch]\n",
        "      \n",
        "      sent_id, mask, labels = batch\n",
        "      val_label.append(labels.detach().cpu().numpy())\n",
        "      \n",
        "      with torch.no_grad():\n",
        "        output = model(sent_id,mask)\n",
        "        \n",
        "        loss = loss_fn(output,labels.unsqueeze(1).float())     \n",
        "\n",
        "        total_loss = total_loss + loss.item()\n",
        "        \n",
        "      pred = torch.round(output)\n",
        "      pred = pred.detach().cpu().numpy()\n",
        "      pred = pred.flatten()\n",
        "      val_preds.append(pred)\n",
        "\n",
        "    # compute training loss of each epoch\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "\n",
        "    # flatten labels and predictions\n",
        "    flat_label = np.concatenate(val_label).astype(int).ravel().tolist()\n",
        "    \n",
        "    \n",
        "    flat_predictions = np.concatenate(val_preds).astype(int).ravel().tolist()\n",
        "\n",
        "    \n",
        "\n",
        "    return flat_label,flat_predictions, avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-rwlDQMpGHR"
      },
      "source": [
        "<h1>F1 Score per Class</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QXwxOERRLqc"
      },
      "source": [
        "def Class_F1_score(val_y,new_preds):\n",
        "\n",
        "  report = classification_report(val_y,new_preds, output_dict=True )\n",
        "\n",
        "  return report['0']['f1-score'], report['1']['f1-score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnAJp5h0pKfm"
      },
      "source": [
        "<h1>Language Model</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFRTq7u_RNyf"
      },
      "source": [
        "def language_model(lang):\n",
        "\n",
        "  if lang=='multilingual_roberta':\n",
        "    return 'xlm-roberta-base'\n",
        "  \n",
        "  else:\n",
        "    return \"dkleczek/bert-base-polish-uncased-v1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T21KkGspP4f"
      },
      "source": [
        "<h1>Balancing with kmedoid</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcKtne5hPy1U"
      },
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "import numpy as np\n",
        "import random\n",
        "from past.builtins import xrange\n",
        "\n",
        "def kMedoids(D, k, tmax=100):\n",
        "    # determine dimensions of distance matrix D\n",
        "    m, n = D.shape\n",
        "\n",
        "    if k > n:\n",
        "        raise Exception('too many medoids')\n",
        "\n",
        "    # find a set of valid initial cluster medoid indices since we\n",
        "    # can't seed different clusters with two points at the same location\n",
        "    valid_medoid_inds = set(range(n))\n",
        "    invalid_medoid_inds = set([])\n",
        "    rs,cs = np.where(D==0)\n",
        "    # the rows, cols must be shuffled because we will keep the first duplicate below\n",
        "    index_shuf = list(range(len(rs)))\n",
        "    np.random.shuffle(index_shuf)\n",
        "    rs = rs[index_shuf]\n",
        "    cs = cs[index_shuf]\n",
        "    for r,c in zip(rs,cs):\n",
        "        # if there are two points with a distance of 0...\n",
        "        # keep the first one for cluster init\n",
        "        if r < c and r not in invalid_medoid_inds:\n",
        "            invalid_medoid_inds.add(c)\n",
        "    valid_medoid_inds = list(valid_medoid_inds - invalid_medoid_inds)\n",
        "\n",
        "    if k > len(valid_medoid_inds):\n",
        "        raise Exception('too many medoids (after removing {} duplicate points)'.format(\n",
        "            len(invalid_medoid_inds)))\n",
        "\n",
        "    # randomly initialize an array of k medoid indices\n",
        "    M = np.array(valid_medoid_inds)\n",
        "    np.random.shuffle(M)\n",
        "    M = np.sort(M[:k])\n",
        "\n",
        "    # create a copy of the array of medoid indices\n",
        "    Mnew = np.copy(M)\n",
        "\n",
        "    # initialize a dictionary to represent clusters\n",
        "    C = {}\n",
        "    for t in xrange(tmax):\n",
        "        # determine clusters, i. e. arrays of data indices\n",
        "        J = np.argmin(D[:,M], axis=1)\n",
        "        for kappa in range(k):\n",
        "            C[kappa] = np.where(J==kappa)[0]\n",
        "        # update cluster medoids\n",
        "        for kappa in range(k):\n",
        "            J = np.mean(D[np.ix_(C[kappa],C[kappa])],axis=1)\n",
        "            j = np.argmin(J)\n",
        "            Mnew[kappa] = C[kappa][j]\n",
        "        np.sort(Mnew)\n",
        "        # check for convergence\n",
        "        if np.array_equal(M, Mnew):\n",
        "            break\n",
        "        M = np.copy(Mnew)\n",
        "    else:\n",
        "        # final update of cluster memberships\n",
        "        J = np.argmin(D[:,M], axis=1)\n",
        "        for kappa in range(k):\n",
        "            C[kappa] = np.where(J==kappa)[0]\n",
        "\n",
        "    # return results\n",
        "    return M, C\n",
        "\n",
        "def compare(ratio,data,balance):\n",
        "    class_tags = data['label'].value_counts().to_list()\n",
        "    if ratio>=0.5:\n",
        "        total = balance - min(class_tags)\n",
        "        balance_class_index = class_tags.index(max(class_tags))\n",
        "        class_index = class_tags.index(min(class_tags))\n",
        "\n",
        "        new = data[data['label']==balance_class_index]\n",
        "        print(new['label'].value_counts())\n",
        "\n",
        "        model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "        embeddings = model.encode(new['text'].to_numpy())\n",
        "\n",
        "        D = pairwise_distances(embeddings, metric='cosine')\n",
        "\n",
        "        M, C = kMedoids(D, total)\n",
        "        data1 = new.iloc[M]\n",
        "    \n",
        "        data2 = data[data['label']==class_index]\n",
        "        return pd.concat([data1,data2],ignore_index=True)\n",
        "    else:\n",
        "      model = SentenceTransformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "      embeddings = model.encode(data['text'].to_numpy())\n",
        "\n",
        "      D = pairwise_distances(embeddings, metric='cosine')\n",
        "\n",
        "      M, C = kMedoids(D, balance)\n",
        "      return data.iloc[M]\n",
        "\n",
        "\n",
        "def check_imbalance_ratio(data):\n",
        "    class_tags = data['label'].value_counts().to_list()\n",
        "    difference = max(class_tags) - min(class_tags)\n",
        "    ratio = difference/max(class_tags)\n",
        "    return ratio\n",
        "\n",
        "def balance(data,sample):\n",
        "    return compare(check_imbalance_ratio(data),data,sample)\n",
        "\n",
        "def execute(data,sample):\n",
        "   \n",
        "    return balance(data,sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7QOp-rdpeWJ"
      },
      "source": [
        "<h1>Choose dataset for Training</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F54V2TyR3HA"
      },
      "source": [
        "# Slovenian\n",
        "text_path = \"/content/drive/My Drive/Datasets/Slovenian/Slovene_train_set.csv\"\n",
        "\n",
        "slovenian = Slovenian(text_path)\n",
        "slovenian_data = slovenian.clean_data()\n",
        "\n",
        "# Polish\n",
        "text_path = \"/content/drive/My Drive/Datasets/Polish/training_set_clean_only_text.txt\"\n",
        "tag_path = \"/content/drive/My Drive/Datasets/Polish/training_set_clean_only_tags.txt\"\n",
        "\n",
        "polish = Polish(text_path,tag_path)\n",
        "polish_data = polish.clean_data()\n",
        "polish_data  = execute(polish_data,7990)\n",
        "\n",
        "# Croatian\n",
        "text_path = \"/content/drive/My Drive/Datasets/Croatian/Croatian_train_set.csv\"\n",
        "croatian = Croatian(text_path)\n",
        "croatian_data = croatian.clean_data()\n",
        "croatian_data  = execute(croatian_data,7990)\n",
        "\n",
        "# Greek\n",
        "text_path = \"/content/drive/My Drive/Datasets/Greek/offenseval-gr_train.csv\"\n",
        "\n",
        "greek = Greek(text_path)\n",
        "greek_data = greek.clean_data()\n",
        "greek_data  = execute(greek_data,7990)\n",
        "\n",
        "# English\n",
        "text_path = \"/content/drive/My Drive/Datasets/English/English_8743_train.csv\"\n",
        "\n",
        "english = English(text_path)\n",
        "english_data = english.clean_data()\n",
        "english_data  = execute(english_data,7990)\n",
        "\n",
        "# Conact Data\n",
        "data = pd.concat([croatian_data,polish_data,slovenian_data,english_data,greek_data])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktU2C51upy6K"
      },
      "source": [
        "<h1>Cross Validation</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pksc5Y_R6h-"
      },
      "source": [
        "kfold=KFold(n_splits=5,shuffle=True,random_state=23)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "accuracy = []\n",
        "fold_loss_train = []\n",
        "fold_loss_val = []\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "language = 'multilingual_roberta'\n",
        "Bert_model = language_model(language)\n",
        "\n",
        "\n",
        "for tidx,vidx in kfold.split(data):\n",
        "  \n",
        "  train_dataset, val_dataset = data.iloc[tidx], data.iloc[vidx]\n",
        "  print(len(train_dataset))\n",
        "  print(len(val_dataset))\n",
        "  \n",
        "  # Transfom Data\n",
        "  transform = Transform_Data(train_dataset,val_dataset,Bert_model,'text','label')\n",
        "  train_encodings, train_y, val_encodings,val_y, train_dataloader, val_dataloader = transform.Execute()\n",
        "\n",
        "  # Bert Model\n",
        "  bert = XLMRobertaModel.from_pretrained(Bert_model)\n",
        "  model = ROBERTA_Arch(bert,False)\n",
        "  model.to(device)\n",
        "\n",
        "  # Optimizer and Scheduler\n",
        "  optimizer, lr_scheduler, num_training_steps = Optimizer_Scheduler(model,train_dataloader,num_epochs)\n",
        "\n",
        "  # Train Model\n",
        "  train_loss,val_loss, total_loss_train, total_loss_val, best_valid_loss, new = train(model, num_epochs, train_dataloader,val_dataloader,best_valid_loss,language)\n",
        "  fold_loss_train.append(total_loss_train)\n",
        "  fold_loss_val.append(total_loss_val)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIScla6Up9vl"
      },
      "source": [
        "<h1>Predict class</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irJm2GqZSLre"
      },
      "source": [
        "def predict(model, test_dataloader):\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    # File in drive\n",
        "    path = '/content/drive/My Drive/Datasets/All/best_multilingual_roberta_weights.pt'\n",
        "    model.load_state_dict(torch.load(path))\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "            #preds = logits.detach().cpu().numpy()\n",
        "            \n",
        "        all_logits.append(torch.round(logits))\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "    preds = all_logits.detach().cpu().numpy()\n",
        "    preds = np.concatenate(preds).astype(int).ravel().tolist()\n",
        "    \n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    #probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoKHNHjuqTyN"
      },
      "source": [
        "<h1>Choose Language</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gchdhaUgkbDx"
      },
      "source": [
        "def choose_language(clanguage):\n",
        "\n",
        "  if clanguage=='Slovenian' or clanguage=='slovenian':\n",
        "    # Test path Slovenian\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Slovenian/Slovene_test_set.csv\"\n",
        "    \n",
        "    # Clean Slovenian data\n",
        "    slovenian = Slovenian(text_path)\n",
        "    test_data = slovenian.clean_data()\n",
        "\n",
        "  elif clanguage=='Polish' or clanguage=='polish':\n",
        "    # Text path Polish\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Polish/test_set_clean_only_text.txt\"\n",
        "    tag_path = \"/content/drive/My Drive/Datasets/Polish/test_set_clean_only_tags.txt\"\n",
        "    \n",
        "    # Clean Polish data\n",
        "    polish = Polish(text_path,tag_path)\n",
        "    test_data = polish.clean_data()\n",
        "\n",
        "  elif clanguage=='Croatian' or clanguage=='croatian':\n",
        "    # Text path Croatian\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Croatian/Croatian_test_set.csv\"\n",
        "    \n",
        "    # Clean Croatian Data\n",
        "    croatian = Croatian(text_path)\n",
        "    test_data = croatian.clean_data()\n",
        "  \n",
        "  elif clanguage=='Greek' or clanguage=='greek':\n",
        "    # Text path Greek\n",
        "    text_path = \"/content/drive/My Drive/Datasets/Greek/offenseval-gr-test.csv\"\n",
        "    \n",
        "    # Clean data\n",
        "    greek = Greek(text_path)\n",
        "    test_data = greek.clean_data()\n",
        "\n",
        "  return test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suDEh2uNqYEn"
      },
      "source": [
        "<h1>Make Predictions</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r1917d8SMSp"
      },
      "source": [
        "import numpy\n",
        "import torch, gc\n",
        "\n",
        "language = 'multilingual_roberta'\n",
        "Bert_model = 'xlm-roberta-base'\n",
        "\n",
        "# Choose Testing dataset\n",
        "test_data = choose_language('polish')\n",
        "\n",
        "# Tokenize and Encode Data\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(Bert_model)\n",
        "test_encodings = tokenizer.batch_encode_plus(test_data['text'].tolist(),add_special_tokens = True, truncation=True, padding=True, max_length=60,return_tensors='pt')\n",
        "\n",
        "# Convert input_ids and attention_mask to tensors\n",
        "test_seq = torch.tensor(test_encodings['input_ids'])\n",
        "test_mask = torch.tensor(test_encodings['attention_mask'])\n",
        "\n",
        "# Sample data\n",
        "test_dataset = TensorDataset(test_seq, test_mask)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\n",
        "\n",
        "test_y = torch.tensor(test_data['label'].tolist())\n",
        "\n",
        "# Model\n",
        "bert = XLMRobertaModel.from_pretrained(Bert_model)\n",
        "model = ROBERTA_Arch(bert,False)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Predict\n",
        "test_pred = predict(model, test_dataloader)\n",
        "test_label = test_y.detach().cpu().numpy()\n",
        "\n",
        "# Print Results\n",
        "print(test_label)\n",
        "print(len(test_label))\n",
        "print(len(test_pred))\n",
        "print(classification_report(test_label,test_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODa11uioE6SE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}