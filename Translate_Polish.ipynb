{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translate.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMhGgJoog/ePwm3cbL+3emp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlavrent/Multilingual-Hate-Speech-Detection/blob/main/Translate_Polish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkTBAIfHVfY5"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece"
      ],
      "metadata": {
        "id": "ytilkvknVkvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XC5f4sHbVnER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "  print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "xS7GZG52Vrip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#from torchsampler import ImbalancedDatasetSampler\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "from sklearn.model_selection import KFold\n",
        "import torch, gc\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from sklearn.metrics import f1_score,classification_report\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n"
      ],
      "metadata": {
        "id": "evGczaMJVt09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Preprocess Polish</h1>"
      ],
      "metadata": {
        "id": "WrHKZcklfGyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Polish():\n",
        "\n",
        "    def __init__(self,data_path,tag_path):\n",
        "        self.path = data_path\n",
        "        self.tag_path = tag_path\n",
        "        self.data = self.read_data()\n",
        "        self.tag = self.read_tag()\n",
        "        self.column = 'text'\n",
        "        self.label = 'label'\n",
        "\n",
        "    def read_data(self):\n",
        "\n",
        "        open_data = open(self.path, \"r\", encoding=\"utf8\")\n",
        "\n",
        "        return pd.DataFrame(open_data)\n",
        "\n",
        "    def read_tag(self):\n",
        "\n",
        "        open_tag = open(self.tag_path,'r')\n",
        "\n",
        "        return pd.DataFrame(open_tag)\n",
        "\n",
        "    def remove_punctuation(self,data,column):\n",
        "      \n",
        "      return data[column].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\n",
        "  \n",
        "    def lower(self):\n",
        "\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def rename_columns(self,data,column):\n",
        "\n",
        "        return data.rename(columns={0:column})\n",
        "\n",
        "    def remove_mentions(self):\n",
        "\n",
        "        return self.data[self.column].apply(lambda row: re.sub(\"@[A-Za-z0-9]+_[A-Za-z0-9]+\",\"\",row))\n",
        "\n",
        "    def remove_end_line(self,data,column):\n",
        "\n",
        "        return data[column].str.replace('\\n','')\n",
        "\n",
        "    def concat(self):\n",
        "\n",
        "        self.data[self.label] = self.tag[self.label]\n",
        "\n",
        "        return self.data\n",
        "    \n",
        "    def convert_int(self):\n",
        "\n",
        "      return self.tag[self.label].apply(lambda x: int(x))\n",
        "\n",
        "\n",
        "    def clean_data(self):\n",
        "\n",
        "        text_column = self.column\n",
        "        label_column = self.label\n",
        "\n",
        "        # Rename columns in both label and text data\n",
        "        self.data = self.rename_columns(self.data,text_column)\n",
        "\n",
        "        self.tag = self.rename_columns(self.tag, label_column)\n",
        "\n",
        "        # Remove Punctuation\n",
        "        self.data[text_column] = self.remove_punctuation(self.data,text_column) \n",
        "\n",
        "        # Lower words in text data\n",
        "        self.data[text_column] = self.lower()\n",
        "\n",
        "        # Remove user mentions in text data\n",
        "        self.data[text_column] = self.remove_mentions()\n",
        "\n",
        "        # Remove end line character from label and text data\n",
        "        self.data[text_column] = self.remove_end_line(self.data,text_column)\n",
        "\n",
        "        self.tag[label_column] = self.remove_end_line(self.tag,label_column)\n",
        "\n",
        "        # Convert label to int\n",
        "        self.tag[label_column] = self.convert_int()\n",
        "        \n",
        "        # Concat text and labels\n",
        "\n",
        "        return self.concat()"
      ],
      "metadata": {
        "id": "U7RP7e-sgE4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Preprocess English</h1>"
      ],
      "metadata": {
        "id": "rMxEZKp_fBqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class English():\n",
        "\n",
        "    def __init__(self,path):\n",
        "        self.data = self.read(path)\n",
        "        self.label = 'label'\n",
        "        self.column = 'text'\n",
        "\n",
        "    def read(self,path):\n",
        "        return pd.read_csv(path)\n",
        "\n",
        "    def replace_label(self,x):\n",
        "        if ('normal' in x) or (x==2):\n",
        "            return 'NOT'\n",
        "        else:\n",
        "            return 'HOF'\n",
        "\n",
        "    def fix_label(self):\n",
        "        self.data[self.label] = self.data[self.label].apply(lambda x: self.replace_label(x))\n",
        "        return self.data\n",
        "\n",
        "\n",
        "    def replace_mentions(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda row: re.sub(\"@[A-Za-z0-9]+_*[A-Za-z0-9]+\", \"mention\", row))\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda row: re.sub(\"mention_\", \"mention\", row))\n",
        "\n",
        "    def remove_punctuation(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "    def replace_hashtag(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(\"#[\\w]+\", \"hashtag\", x))\n",
        "\n",
        "    def remove_stopwords(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda word: [i for i in word.split() if not i in stopwords.words(\"english\")])\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    def remove_url(self):\n",
        "        self.data[self.column] = self.data[self.column].apply(lambda x: re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '',x))\n",
        "    \n",
        "    def lower(self):\n",
        "        return self.data[self.column].str.lower()\n",
        "\n",
        "    def binarize_labels(self):\n",
        "\n",
        "      self.data[self.label] = self.data[self.label].apply(lambda x: 0 if x=='NOT' else 1)\n",
        "\n",
        "    def clean_data(self):\n",
        "        # Fix labels\n",
        "        self.data = self.fix_label()\n",
        "\n",
        "        # Remove urls\n",
        "        self.remove_url()\n",
        "\n",
        "        # Replace mentions\n",
        "        self.replace_mentions()\n",
        "\n",
        "        # Replace hashtags\n",
        "        self.replace_hashtag()\n",
        "\n",
        "        # Remove punctuation\n",
        "        self.remove_punctuation()\n",
        "\n",
        "        # Remove stopwords\n",
        "        self.remove_stopwords()\n",
        "\n",
        "        # Lower text\n",
        "        self.lower()\n",
        "\n",
        "        # Binarize labels\n",
        "        self.binarize_labels()\n",
        "\n",
        "        return self.data"
      ],
      "metadata": {
        "id": "iuQgmHJEgK9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Load English Data</h1>"
      ],
      "metadata": {
        "id": "UuIL9PVFe6qX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text_path = \"/content/drive/My Drive/Datasets/English/English_train_set.csv\"\n",
        "\n",
        "english = English(text_path)\n",
        "data = english.fix_label()\n",
        "data['label'] = data['label'].apply(lambda x: 1 if x=='HOF' else 0)"
      ],
      "metadata": {
        "id": "F2MEVxpDgN1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Find imbalanced class</h1>"
      ],
      "metadata": {
        "id": "Ozfx27KdfSRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_path = \"/content/drive/My Drive/Datasets/Polish/training_set_clean_only_text.txt\"\n",
        "tag_path = \"/content/drive/My Drive/Datasets/Polish/training_set_clean_only_tags.txt\"\n",
        "\n",
        "# Clean Polish data (labels)\n",
        "polish = Polish(text_path,tag_path)\n",
        "data_p = polish.clean_data()\n",
        "\n",
        "# Find difference of samples in both classes\n",
        "classes = data_p['label'].value_counts().to_list()\n",
        "abs(classes[0]-classes[1])\n",
        "\n",
        "# Find Imbalance in classes\n",
        "print(data_p['label'].value_counts())"
      ],
      "metadata": {
        "id": "vVyJ9sALgRaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class_tags = data_p['label'].value_counts().to_list()\n",
        "\n",
        "# Find class to balance\n",
        "balance_class_index = class_tags.index(min(class_tags))\n",
        "class_index = data_p['label'].value_counts().index.to_list()\n",
        "\n",
        "# Find class not to balance\n",
        "balance_class_index_not = class_tags.index(max(class_tags))\n",
        "class_index_not = data_p['label'].value_counts().index.to_list()\n",
        "\n",
        "# Choose class to sample\n",
        "sample_data = data[data['label']==class_index[balance_class_index]]\n",
        "\n",
        "# Choose class not to sample\n",
        "not_sample_data = data[data['label']==class_index_not[balance_class_index_not]]\n",
        "\n",
        "# Split into sample to translate and sample to not translate\n",
        "use_sample ,translate_sample = train_test_split(sample_data,test_size=abs(class_tags[0]-class_tags[1])) \n",
        "\n",
        "# Data to use for training\n",
        "data = pd.concat([use_sample,not_sample_data])"
      ],
      "metadata": {
        "id": "vw8aoSMUgVc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Load Model</h1>"
      ],
      "metadata": {
        "id": "4eakb2BaffIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "\n",
        "# Load Model\n",
        "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
        "\n",
        "# Model to GPU\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "spgE7cjYgqOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Translate in Batches</h1>"
      ],
      "metadata": {
        "id": "A31y7V_2f0I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a part of the data \n",
        "new = translate_sample[:200]\n",
        "\n",
        "# Tokenize data\n",
        "encoded_hi = tokenizer(new['text'].tolist(), return_tensors=\"pt\",padding=True)\n",
        "\n",
        "# Create DataLoader\n",
        "test_dataset = TensorDataset(encoded_hi['input_ids'],encoded_hi['attention_mask'])\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset,sampler=test_sampler,batch_size=8)\n",
        "\n",
        "\n",
        "translated_text = []\n",
        "i = 1\n",
        "\n",
        "# Translate in batches\n",
        "for batch in test_dataloader:\n",
        "\n",
        "  b_input,b_attn_mask = [t.to(device) for t in batch]\n",
        "  \n",
        "  generated_tokens = model.generate(b_input, forced_bos_token_id=tokenizer.get_lang_id(\"pl\"))\n",
        "  encoded = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "  pd.DataFrame(encoded).to_csv('/content/drive/My Drive/Datasets/Polish/Batch_'+str(i)+'.csv')\n",
        "  i = i + 1\n"
      ],
      "metadata": {
        "id": "7IJbo1rreaa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1></h1>"
      ],
      "metadata": {
        "id": "waS_4LhtjAaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Concat csv files into one</h1>"
      ],
      "metadata": {
        "id": "ENzNPdZbjJXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "import pandas as pd\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Datasets\")\n",
        "add = []\n",
        "for file in glob.glob(\"Batch_*.csv\"):\n",
        "  add.append(pd.read_csv(file))\n",
        "\n",
        "pd.concat(add).to_csv('/content/drive/My Drive/Datasets/Translated_Polish.csv',index=False)"
      ],
      "metadata": {
        "id": "_82MhH4rlaW8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}